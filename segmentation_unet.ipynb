{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image segmentation with pytorch using U-net\n",
    "\n",
    "U-net was first developed in 2015 by Ronneberger et al., as a segmentation network for biomedical image analysis.\n",
    "It has been extremely successful, with 9,000+ citations, and many new methods that have used the U-net architecture since.\n",
    "\n",
    "\n",
    "The architecture of U-net is based on the idea of using skip connections (i.e. concatenating) at different levels of the network to retain high, and low level features.\n",
    "\n",
    "Here is the architecture of a U-net:\n",
    "\n",
    "---\n",
    "\n",
    "![U-net](unet.png)\n",
    "Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-photon microscopy dataset of cortical axons\n",
    "\n",
    "In this tutorial we use a dataset of cortical neurons with their corresponding segmentation binary labels.\n",
    "\n",
    "These images were collected using in-vivo two-photon microscopy from the mouse somatosensory cortex. To generate the 2D images, a max projection was used over the 3D stack. The labels are binary segmentation maps of the axons.\n",
    "\n",
    "Here we will use 100 [64x64] crops during training and validation. \n",
    "\n",
    "These are some example images [256x256] from the original dataset:\n",
    "![axon_dataset](axon_dataset.png)\n",
    "\n",
    "Bass, Cher, et al. \"Image synthesis with a convolutional capsule generative adversarial network.\" Medical Imaging with Deep Learning (2019).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load modules\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from load_memmap import *\n",
    "from AxonDataset import *\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "timestr = time.strftime(\"%d%m%Y-%H%M\")\n",
    "__location__ = os.path.realpath(\n",
    "    os.path.join(os.getcwd(), os.path.dirname('__file__')))\n",
    "\n",
    "path = os.path.join(__location__,'results')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "# Define your batch_size\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataloader\n",
    "\n",
    "In this example, a custom dataloader was created, and we import it from `AxonDataset.py`\n",
    "\n",
    "we create a dataset, and split into a train and validation set with 80%, 20% split\n",
    "\n",
    "### Task 1\n",
    "\n",
    "create a list of random indices for the train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we create a dataloader for our example dataset- two photon microscopy with axons\n",
    "axon_dataset = AxonDataset(data_name='org64', type='train')\n",
    "\n",
    "# -----------------------------------------------------task 1----------------------------------------------------------------\n",
    "# Task 1: create a random list of incides for training and testing with a 80%,20% split\n",
    "\n",
    "# We need to further split our training dataset into training and validation sets.\n",
    "# Define the indices\n",
    "indices = list(range(len(axon_dataset)))  # start with all the indices in training set\n",
    "split = int(len(indices)*0.2)  # define the split size\n",
    "\n",
    "# Get indices for train and validation datasets, and split the data\n",
    "validation_idx = np.random.choice(indices, size=split, replace=False)\n",
    "train_idx = list(set(indices) - set(validation_idx))\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# feed indices into the sampler\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "\n",
    "# Create a dataloader instance \n",
    "train_loader = torch.utils.data.DataLoader(axon_dataset, batch_size = batch_size,\n",
    "                                           sampler=train_sampler) \n",
    "val_loader = torch.utils.data.DataLoader(axon_dataset, batch_size = batch_size,\n",
    "                                        sampler=validation_sampler) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a U-net \n",
    "\n",
    "We next build our u-net network.\n",
    "\n",
    "First we define a layer `double_conv` that performs 2 sets of convolution followed by ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define U-net\n",
    "def double_conv(in_channels, out_channels, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=padding),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define neural network\n",
    "We then define our U-net network.\n",
    "\n",
    "We initialise all the different layers in the network in `__init__`:\n",
    "1. `self.dconv_down1` is a double convolutional layer\n",
    "2. `self.maxpool` is a max pooling layer that is used to reduce the size of the input, and decrease the reptive field\n",
    "3. `self.upsample` is an upsampling layer that is used to increase the size of the input\n",
    "4. `dropout` is a dropout layer that is applied to regulise the training\n",
    "5. `dconv_up4` is also a double convolutional layer- note that it takes in additional channels from previous layers (i.e. the skip connections).\n",
    "\n",
    "skip connection are easily implemented by concatenating the result of a previous convolution with the current input, \n",
    "\n",
    "using e.g. `torch.cat([x, conv4], dim=1)`\n",
    "\n",
    "### Task 2 - implement skip connections\n",
    "Implement skip connections for conv3, conv2, and conv1.\n",
    "\n",
    "See conv4 example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dconv_down1 = double_conv(1, 32)\n",
    "        self.dconv_down2 = double_conv(32, 64)\n",
    "        self.dconv_down3 = double_conv(64, 128)\n",
    "        self.dconv_down4 = double_conv(128, 256)\n",
    "\n",
    "        self.dconv_down5 = double_conv(256, 512)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.dconv_up4 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up3 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up2 = double_conv(128 + 64, 64)\n",
    "        self.dconv_up1 = double_conv(64 + 32, 32)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(32, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        conv1 = self.dropout(conv1)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        conv2 = self.dropout(conv2)\n",
    "        x = self.maxpool(conv2)\n",
    "\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        conv3 = self.dropout(conv3)\n",
    "        x = self.maxpool(conv3)\n",
    "\n",
    "        conv4 = self.dconv_down4(x)\n",
    "        conv4 = self.dropout(conv4)\n",
    "        x = self.maxpool(conv4)\n",
    "\n",
    "        conv5 = self.dconv_down5(x)\n",
    "        conv5 = self.dropout(conv5)\n",
    "\n",
    "        x = self.upsample(conv5)\n",
    "        \n",
    "        # example of skip connection with conv4\n",
    "        x = torch.cat([x, conv4], dim=1)\n",
    "        \n",
    "        x = self.dconv_up4(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        # --------------------------------------------------- task 2 ----------------------------------------------------------\n",
    "        # Task 2: implement skip connection with conv3\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        # --------------------------------------------------- task 2 ----------------------------------------------------------\n",
    "        # Task 2: implement skip connection with conv2\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        # --------------------------------------------------- task 2 ----------------------------------------------------------\n",
    "        # Task 2: implement skip connection with conv1\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        out = F.sigmoid(self.conv_last(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we initialise the network with a previously trained network by loading the weights\n",
    "\n",
    "*for practical reasons training this network from scratch will take too long, and require large computational resources*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise network - and load weights\n",
    "net = UNet()\n",
    "net.load_state_dict(torch.load(path+'/'+'model.pt')) #this function loads a pretrained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an appropriate loss function\n",
    "We next define our loss function - in this case we use Dice loss, a commonly used loss for image segmentation.\n",
    "\n",
    "The Dice coefficient can be used as a loss function, and is essentially a measure of overlap between two samples.\n",
    "\n",
    "Dice is in the range of 0 to 1, where a Dice coefficient of 1 denotes perfect and complete overlap. The Dice coefficient was originally developed for binary data, and can be calculated as:\n",
    "\n",
    "$Dice = \\dfrac{2|A\\cap B|}{|A| + |B|}$\n",
    "\n",
    "where $|A\\cap B|$ represents the common elements between sets $A$ and $B$, and $|A|$ represents the number of elements in set $A$ (and likewise for set $B$).\n",
    "\n",
    "For the case of evaluating a Dice coefficient on predicted segmentation masks, we can approximate  $|A\\cap B|$ as the element-wise multiplication between the prediction and target mask, and then sum the resulting matrix.\n",
    "\n",
    "An **alternative loss** function would be pixel-wise cross entropy loss. It would examine each pixel individually, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice loss\n",
    "def dice_coeff(pred, target):\n",
    "    \"\"\"This definition generalize to real valued pred and target vector.\n",
    "    This should be differentiable.\n",
    "    pred: tensor with first dimension as batch\n",
    "    target: tensor with first dimension as batch\n",
    "    \"\"\"\n",
    "\n",
    "    smooth = 1.\n",
    "    epsilon = 10e-8\n",
    "\n",
    "    # have to use contiguous since they may from a torch.view op\n",
    "    iflat = pred.contiguous().view(-1)\n",
    "    tflat = target.contiguous().view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    A_sum = torch.sum(iflat * iflat)\n",
    "    B_sum = torch.sum(tflat * tflat)\n",
    "\n",
    "    dice = (2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "    dice = dice.mean(dim=0)\n",
    "    dice = torch.clamp(dice, 0, 1.0-epsilon)\n",
    "\n",
    "    return  dice\n",
    "\n",
    "# cross entropy loss\n",
    "loss_BCE = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as before, we define the optimiser to train our network - here we use Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define your optimiser\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=1e-05, betas=(0.5, 0.999))\n",
    "optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating our segmentation network\n",
    "We next train and evaluate our network \n",
    "\n",
    "note that the results are saved to a folder \\results - so please check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cb19/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/home/cb19/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][0/20] Elapsed_time: 0m1s Loss: 0.6969 Dice: 0.6165\n",
      "[0/10][1/20] Elapsed_time: 0m2s Loss: 0.6991 Dice: 0.5887\n",
      "[0/10][2/20] Elapsed_time: 0m3s Loss: 0.6957 Dice: 0.5749\n",
      "[0/10][3/20] Elapsed_time: 0m3s Loss: 0.6987 Dice: 0.6007\n",
      "[0/10][4/20] Elapsed_time: 0m4s Loss: 0.6915 Dice: 0.5546\n",
      "[0/10][5/20] Elapsed_time: 0m5s Loss: 0.6968 Dice: 0.5634\n",
      "[0/10][6/20] Elapsed_time: 0m6s Loss: 0.6948 Dice: 0.5440\n",
      "[0/10][7/20] Elapsed_time: 0m7s Loss: 0.6936 Dice: 0.4862\n",
      "[0/10][8/20] Elapsed_time: 0m8s Loss: 0.6914 Dice: 0.5159\n",
      "[0/10][9/20] Elapsed_time: 0m9s Loss: 0.6899 Dice: 0.4977\n",
      "[0/10][10/20] Elapsed_time: 0m10s Loss: 0.6913 Dice: 0.4320\n",
      "[0/10][11/20] Elapsed_time: 0m11s Loss: 0.6924 Dice: 0.3564\n",
      "[0/10][12/20] Elapsed_time: 0m12s Loss: 0.6918 Dice: 0.4181\n",
      "[0/10][13/20] Elapsed_time: 0m13s Loss: 0.6894 Dice: 0.4948\n",
      "[0/10][14/20] Elapsed_time: 0m14s Loss: 0.6942 Dice: 0.4425\n",
      "[0/10][15/20] Elapsed_time: 0m15s Loss: 0.6889 Dice: 0.4144\n",
      "[0/10][16/20] Elapsed_time: 0m17s Loss: 0.6903 Dice: 0.4661\n",
      "[0/10][17/20] Elapsed_time: 0m18s Loss: 0.6905 Dice: 0.4305\n",
      "[0/10][18/20] Elapsed_time: 0m19s Loss: 0.6922 Dice: 0.4261\n",
      "[0/10][19/20] Elapsed_time: 0m20s Loss: 0.6918 Dice: 0.3204\n",
      "Elapsed_time: 0m1s Val dice: 0.5260\n",
      "[1/10][0/20] Elapsed_time: 0m1s Loss: 0.6926 Dice: 0.3236\n",
      "[1/10][1/20] Elapsed_time: 0m2s Loss: 0.6906 Dice: 0.5381\n",
      "[1/10][2/20] Elapsed_time: 0m3s Loss: 0.6918 Dice: 0.3898\n",
      "[1/10][3/20] Elapsed_time: 0m4s Loss: 0.6917 Dice: 0.2835\n",
      "[1/10][4/20] Elapsed_time: 0m5s Loss: 0.6894 Dice: 0.3513\n",
      "[1/10][5/20] Elapsed_time: 0m6s Loss: 0.6917 Dice: 0.4037\n",
      "[1/10][6/20] Elapsed_time: 0m7s Loss: 0.6920 Dice: 0.4402\n",
      "[1/10][7/20] Elapsed_time: 0m8s Loss: 0.6910 Dice: 0.4512\n",
      "[1/10][8/20] Elapsed_time: 0m9s Loss: 0.6906 Dice: 0.5261\n",
      "[1/10][9/20] Elapsed_time: 0m10s Loss: 0.6911 Dice: 0.4813\n",
      "[1/10][10/20] Elapsed_time: 0m11s Loss: 0.6904 Dice: 0.4085\n",
      "[1/10][11/20] Elapsed_time: 0m12s Loss: 0.6909 Dice: 0.3703\n",
      "[1/10][12/20] Elapsed_time: 0m13s Loss: 0.6922 Dice: 0.3760\n",
      "[1/10][13/20] Elapsed_time: 0m14s Loss: 0.6915 Dice: 0.3854\n",
      "[1/10][14/20] Elapsed_time: 0m15s Loss: 0.6899 Dice: 0.4109\n",
      "[1/10][15/20] Elapsed_time: 0m17s Loss: 0.6917 Dice: 0.4013\n",
      "[1/10][16/20] Elapsed_time: 0m18s Loss: 0.6912 Dice: 0.4032\n",
      "[1/10][17/20] Elapsed_time: 0m19s Loss: 0.6922 Dice: 0.3918\n",
      "[1/10][18/20] Elapsed_time: 0m20s Loss: 0.6910 Dice: 0.3938\n",
      "[1/10][19/20] Elapsed_time: 0m21s Loss: 0.6918 Dice: 0.3960\n",
      "Elapsed_time: 0m1s Val dice: 0.5217\n",
      "[2/10][0/20] Elapsed_time: 0m1s Loss: 0.6894 Dice: 0.4658\n",
      "[2/10][1/20] Elapsed_time: 0m2s Loss: 0.6916 Dice: 0.2942\n",
      "[2/10][2/20] Elapsed_time: 0m3s Loss: 0.6893 Dice: 0.4483\n",
      "[2/10][3/20] Elapsed_time: 0m4s Loss: 0.6919 Dice: 0.4263\n",
      "[2/10][4/20] Elapsed_time: 0m5s Loss: 0.6908 Dice: 0.4331\n",
      "[2/10][5/20] Elapsed_time: 0m6s Loss: 0.6908 Dice: 0.4053\n",
      "[2/10][6/20] Elapsed_time: 0m7s Loss: 0.6889 Dice: 0.4263\n",
      "[2/10][7/20] Elapsed_time: 0m8s Loss: 0.6903 Dice: 0.4095\n",
      "[2/10][8/20] Elapsed_time: 0m9s Loss: 0.6925 Dice: 0.3111\n",
      "[2/10][9/20] Elapsed_time: 0m10s Loss: 0.6917 Dice: 0.4163\n",
      "[2/10][10/20] Elapsed_time: 0m11s Loss: 0.6915 Dice: 0.3739\n",
      "[2/10][11/20] Elapsed_time: 0m12s Loss: 0.6900 Dice: 0.4998\n",
      "[2/10][12/20] Elapsed_time: 0m13s Loss: 0.6930 Dice: 0.3969\n",
      "[2/10][13/20] Elapsed_time: 0m14s Loss: 0.6916 Dice: 0.3595\n",
      "[2/10][14/20] Elapsed_time: 0m15s Loss: 0.6912 Dice: 0.4439\n",
      "[2/10][15/20] Elapsed_time: 0m16s Loss: 0.6881 Dice: 0.4359\n",
      "[2/10][16/20] Elapsed_time: 0m17s Loss: 0.6918 Dice: 0.4152\n",
      "[2/10][17/20] Elapsed_time: 0m18s Loss: 0.6924 Dice: 0.4005\n",
      "[2/10][18/20] Elapsed_time: 0m19s Loss: 0.6913 Dice: 0.4899\n",
      "[2/10][19/20] Elapsed_time: 0m21s Loss: 0.6911 Dice: 0.4525\n",
      "Elapsed_time: 0m1s Val dice: 0.5144\n",
      "[3/10][0/20] Elapsed_time: 0m1s Loss: 0.6906 Dice: 0.4034\n",
      "[3/10][1/20] Elapsed_time: 0m2s Loss: 0.6894 Dice: 0.4378\n",
      "[3/10][2/20] Elapsed_time: 0m3s Loss: 0.6909 Dice: 0.4170\n",
      "[3/10][3/20] Elapsed_time: 0m4s Loss: 0.6919 Dice: 0.4143\n",
      "[3/10][4/20] Elapsed_time: 0m6s Loss: 0.6922 Dice: 0.4342\n",
      "[3/10][5/20] Elapsed_time: 0m7s Loss: 0.6894 Dice: 0.5447\n",
      "[3/10][6/20] Elapsed_time: 0m8s Loss: 0.6935 Dice: 0.3551\n",
      "[3/10][7/20] Elapsed_time: 0m9s Loss: 0.6923 Dice: 0.3309\n",
      "[3/10][8/20] Elapsed_time: 0m10s Loss: 0.6906 Dice: 0.4904\n",
      "[3/10][9/20] Elapsed_time: 0m11s Loss: 0.6895 Dice: 0.3973\n",
      "[3/10][10/20] Elapsed_time: 0m12s Loss: 0.6912 Dice: 0.4191\n",
      "[3/10][11/20] Elapsed_time: 0m13s Loss: 0.6915 Dice: 0.4031\n",
      "[3/10][12/20] Elapsed_time: 0m14s Loss: 0.6925 Dice: 0.3813\n",
      "[3/10][13/20] Elapsed_time: 0m16s Loss: 0.6899 Dice: 0.4061\n",
      "[3/10][14/20] Elapsed_time: 0m17s Loss: 0.6906 Dice: 0.3719\n",
      "[3/10][15/20] Elapsed_time: 0m18s Loss: 0.6912 Dice: 0.3584\n",
      "[3/10][16/20] Elapsed_time: 0m19s Loss: 0.6906 Dice: 0.4273\n",
      "[3/10][17/20] Elapsed_time: 0m20s Loss: 0.6912 Dice: 0.4583\n",
      "[3/10][18/20] Elapsed_time: 0m21s Loss: 0.6896 Dice: 0.4136\n",
      "[3/10][19/20] Elapsed_time: 0m22s Loss: 0.6906 Dice: 0.4128\n",
      "Elapsed_time: 0m1s Val dice: 0.5095\n",
      "[4/10][0/20] Elapsed_time: 0m1s Loss: 0.6925 Dice: 0.3827\n",
      "[4/10][1/20] Elapsed_time: 0m2s Loss: 0.6903 Dice: 0.3414\n",
      "[4/10][2/20] Elapsed_time: 0m3s Loss: 0.6919 Dice: 0.4237\n",
      "[4/10][3/20] Elapsed_time: 0m4s Loss: 0.6911 Dice: 0.3870\n",
      "[4/10][4/20] Elapsed_time: 0m5s Loss: 0.6917 Dice: 0.3300\n",
      "[4/10][5/20] Elapsed_time: 0m7s Loss: 0.6896 Dice: 0.4311\n",
      "[4/10][6/20] Elapsed_time: 0m8s Loss: 0.6901 Dice: 0.4040\n",
      "[4/10][7/20] Elapsed_time: 0m9s Loss: 0.6908 Dice: 0.3678\n",
      "[4/10][8/20] Elapsed_time: 0m10s Loss: 0.6918 Dice: 0.4942\n",
      "[4/10][9/20] Elapsed_time: 0m11s Loss: 0.6922 Dice: 0.3472\n",
      "[4/10][10/20] Elapsed_time: 0m12s Loss: 0.6901 Dice: 0.3332\n",
      "[4/10][11/20] Elapsed_time: 0m13s Loss: 0.6923 Dice: 0.2980\n",
      "[4/10][12/20] Elapsed_time: 0m14s Loss: 0.6900 Dice: 0.4042\n",
      "[4/10][13/20] Elapsed_time: 0m15s Loss: 0.6907 Dice: 0.3724\n",
      "[4/10][14/20] Elapsed_time: 0m17s Loss: 0.6893 Dice: 0.3974\n",
      "[4/10][15/20] Elapsed_time: 0m18s Loss: 0.6919 Dice: 0.4634\n",
      "[4/10][16/20] Elapsed_time: 0m18s Loss: 0.6903 Dice: 0.4434\n",
      "[4/10][17/20] Elapsed_time: 0m20s Loss: 0.6915 Dice: 0.4292\n",
      "[4/10][18/20] Elapsed_time: 0m21s Loss: 0.6886 Dice: 0.4391\n",
      "[4/10][19/20] Elapsed_time: 0m22s Loss: 0.6896 Dice: 0.4679\n",
      "Elapsed_time: 0m1s Val dice: 0.5115\n",
      "[5/10][0/20] Elapsed_time: 0m1s Loss: 0.6898 Dice: 0.3534\n",
      "[5/10][1/20] Elapsed_time: 0m2s Loss: 0.6918 Dice: 0.3704\n",
      "[5/10][2/20] Elapsed_time: 0m3s Loss: 0.6908 Dice: 0.3941\n",
      "[5/10][3/20] Elapsed_time: 0m4s Loss: 0.6917 Dice: 0.3652\n",
      "[5/10][4/20] Elapsed_time: 0m6s Loss: 0.6895 Dice: 0.3225\n",
      "[5/10][5/20] Elapsed_time: 0m7s Loss: 0.6912 Dice: 0.4407\n",
      "[5/10][6/20] Elapsed_time: 0m8s Loss: 0.6918 Dice: 0.3582\n",
      "[5/10][7/20] Elapsed_time: 0m10s Loss: 0.6915 Dice: 0.3982\n",
      "[5/10][8/20] Elapsed_time: 0m11s Loss: 0.6920 Dice: 0.2286\n",
      "[5/10][9/20] Elapsed_time: 0m13s Loss: 0.6897 Dice: 0.4978\n",
      "[5/10][10/20] Elapsed_time: 0m14s Loss: 0.6912 Dice: 0.3727\n",
      "[5/10][11/20] Elapsed_time: 0m15s Loss: 0.6911 Dice: 0.3591\n",
      "[5/10][12/20] Elapsed_time: 0m16s Loss: 0.6896 Dice: 0.4135\n",
      "[5/10][13/20] Elapsed_time: 0m18s Loss: 0.6910 Dice: 0.4769\n",
      "[5/10][14/20] Elapsed_time: 0m19s Loss: 0.6909 Dice: 0.3721\n",
      "[5/10][15/20] Elapsed_time: 0m20s Loss: 0.6908 Dice: 0.4387\n",
      "[5/10][16/20] Elapsed_time: 0m21s Loss: 0.6877 Dice: 0.5335\n",
      "[5/10][17/20] Elapsed_time: 0m22s Loss: 0.6902 Dice: 0.4168\n",
      "[5/10][18/20] Elapsed_time: 0m23s Loss: 0.6879 Dice: 0.5144\n",
      "[5/10][19/20] Elapsed_time: 0m24s Loss: 0.6925 Dice: 0.3413\n",
      "Elapsed_time: 0m1s Val dice: 0.5355\n",
      "[6/10][0/20] Elapsed_time: 0m1s Loss: 0.6911 Dice: 0.4488\n",
      "[6/10][1/20] Elapsed_time: 0m2s Loss: 0.6905 Dice: 0.3847\n",
      "[6/10][2/20] Elapsed_time: 0m3s Loss: 0.6909 Dice: 0.4571\n",
      "[6/10][3/20] Elapsed_time: 0m4s Loss: 0.6909 Dice: 0.4023\n",
      "[6/10][4/20] Elapsed_time: 0m5s Loss: 0.6901 Dice: 0.4377\n",
      "[6/10][5/20] Elapsed_time: 0m6s Loss: 0.6900 Dice: 0.4319\n",
      "[6/10][6/20] Elapsed_time: 0m7s Loss: 0.6906 Dice: 0.4658\n",
      "[6/10][7/20] Elapsed_time: 0m9s Loss: 0.6895 Dice: 0.4392\n",
      "[6/10][8/20] Elapsed_time: 0m10s Loss: 0.6903 Dice: 0.4799\n",
      "[6/10][9/20] Elapsed_time: 0m11s Loss: 0.6895 Dice: 0.3955\n",
      "[6/10][10/20] Elapsed_time: 0m12s Loss: 0.6934 Dice: 0.4840\n",
      "[6/10][11/20] Elapsed_time: 0m13s Loss: 0.6917 Dice: 0.4365\n",
      "[6/10][12/20] Elapsed_time: 0m14s Loss: 0.6882 Dice: 0.4154\n",
      "[6/10][13/20] Elapsed_time: 0m16s Loss: 0.6921 Dice: 0.4081\n",
      "[6/10][14/20] Elapsed_time: 0m17s Loss: 0.6900 Dice: 0.4058\n",
      "[6/10][15/20] Elapsed_time: 0m18s Loss: 0.6917 Dice: 0.4208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10][16/20] Elapsed_time: 0m20s Loss: 0.6922 Dice: 0.3761\n",
      "[6/10][17/20] Elapsed_time: 0m21s Loss: 0.6889 Dice: 0.3886\n",
      "[6/10][18/20] Elapsed_time: 0m22s Loss: 0.6918 Dice: 0.3768\n",
      "[6/10][19/20] Elapsed_time: 0m23s Loss: 0.6899 Dice: 0.4481\n",
      "Elapsed_time: 0m1s Val dice: 0.5256\n",
      "[7/10][0/20] Elapsed_time: 0m1s Loss: 0.6934 Dice: 0.3604\n",
      "[7/10][1/20] Elapsed_time: 0m2s Loss: 0.6921 Dice: 0.3421\n",
      "[7/10][2/20] Elapsed_time: 0m4s Loss: 0.6906 Dice: 0.4212\n",
      "[7/10][3/20] Elapsed_time: 0m4s Loss: 0.6918 Dice: 0.4078\n",
      "[7/10][4/20] Elapsed_time: 0m6s Loss: 0.6887 Dice: 0.2939\n",
      "[7/10][5/20] Elapsed_time: 0m7s Loss: 0.6910 Dice: 0.3273\n",
      "[7/10][6/20] Elapsed_time: 0m8s Loss: 0.6900 Dice: 0.3202\n",
      "[7/10][7/20] Elapsed_time: 0m9s Loss: 0.6916 Dice: 0.3520\n",
      "[7/10][8/20] Elapsed_time: 0m10s Loss: 0.6901 Dice: 0.4015\n",
      "[7/10][9/20] Elapsed_time: 0m12s Loss: 0.6906 Dice: 0.3427\n",
      "[7/10][10/20] Elapsed_time: 0m13s Loss: 0.6892 Dice: 0.4669\n",
      "[7/10][11/20] Elapsed_time: 0m14s Loss: 0.6914 Dice: 0.4335\n",
      "[7/10][12/20] Elapsed_time: 0m15s Loss: 0.6875 Dice: 0.3887\n",
      "[7/10][13/20] Elapsed_time: 0m16s Loss: 0.6908 Dice: 0.4386\n",
      "[7/10][14/20] Elapsed_time: 0m17s Loss: 0.6917 Dice: 0.4917\n",
      "[7/10][15/20] Elapsed_time: 0m18s Loss: 0.6898 Dice: 0.4133\n",
      "[7/10][16/20] Elapsed_time: 0m20s Loss: 0.6896 Dice: 0.5088\n",
      "[7/10][17/20] Elapsed_time: 0m21s Loss: 0.6905 Dice: 0.5312\n",
      "[7/10][18/20] Elapsed_time: 0m22s Loss: 0.6926 Dice: 0.4428\n",
      "[7/10][19/20] Elapsed_time: 0m23s Loss: 0.6919 Dice: 0.4532\n",
      "Elapsed_time: 0m1s Val dice: 0.5256\n",
      "[8/10][0/20] Elapsed_time: 0m1s Loss: 0.6905 Dice: 0.5364\n",
      "[8/10][1/20] Elapsed_time: 0m3s Loss: 0.6897 Dice: 0.4564\n",
      "[8/10][2/20] Elapsed_time: 0m4s Loss: 0.6901 Dice: 0.3872\n",
      "[8/10][3/20] Elapsed_time: 0m5s Loss: 0.6891 Dice: 0.4573\n",
      "[8/10][4/20] Elapsed_time: 0m6s Loss: 0.6903 Dice: 0.3228\n",
      "[8/10][5/20] Elapsed_time: 0m7s Loss: 0.6900 Dice: 0.4459\n",
      "[8/10][6/20] Elapsed_time: 0m8s Loss: 0.6906 Dice: 0.3561\n",
      "[8/10][7/20] Elapsed_time: 0m10s Loss: 0.6917 Dice: 0.4099\n",
      "[8/10][8/20] Elapsed_time: 0m11s Loss: 0.6913 Dice: 0.4151\n",
      "[8/10][9/20] Elapsed_time: 0m12s Loss: 0.6907 Dice: 0.3828\n",
      "[8/10][10/20] Elapsed_time: 0m13s Loss: 0.6898 Dice: 0.4196\n",
      "[8/10][11/20] Elapsed_time: 0m15s Loss: 0.6906 Dice: 0.4463\n",
      "[8/10][12/20] Elapsed_time: 0m16s Loss: 0.6910 Dice: 0.4549\n",
      "[8/10][13/20] Elapsed_time: 0m17s Loss: 0.6926 Dice: 0.3655\n",
      "[8/10][14/20] Elapsed_time: 0m18s Loss: 0.6919 Dice: 0.3990\n",
      "[8/10][15/20] Elapsed_time: 0m19s Loss: 0.6907 Dice: 0.3848\n",
      "[8/10][16/20] Elapsed_time: 0m21s Loss: 0.6887 Dice: 0.4610\n",
      "[8/10][17/20] Elapsed_time: 0m22s Loss: 0.6923 Dice: 0.4804\n",
      "[8/10][18/20] Elapsed_time: 0m23s Loss: 0.6868 Dice: 0.5257\n",
      "[8/10][19/20] Elapsed_time: 0m24s Loss: 0.6892 Dice: 0.4377\n",
      "Elapsed_time: 0m1s Val dice: 0.5280\n",
      "[9/10][0/20] Elapsed_time: 0m1s Loss: 0.6910 Dice: 0.4358\n",
      "[9/10][1/20] Elapsed_time: 0m2s Loss: 0.6906 Dice: 0.3739\n",
      "[9/10][2/20] Elapsed_time: 0m3s Loss: 0.6923 Dice: 0.3547\n",
      "[9/10][3/20] Elapsed_time: 0m4s Loss: 0.6902 Dice: 0.4018\n",
      "[9/10][4/20] Elapsed_time: 0m5s Loss: 0.6923 Dice: 0.3252\n",
      "[9/10][5/20] Elapsed_time: 0m6s Loss: 0.6892 Dice: 0.4689\n",
      "[9/10][6/20] Elapsed_time: 0m7s Loss: 0.6904 Dice: 0.4916\n",
      "[9/10][7/20] Elapsed_time: 0m8s Loss: 0.6919 Dice: 0.3625\n",
      "[9/10][8/20] Elapsed_time: 0m10s Loss: 0.6895 Dice: 0.4117\n",
      "[9/10][9/20] Elapsed_time: 0m11s Loss: 0.6905 Dice: 0.4799\n",
      "[9/10][10/20] Elapsed_time: 0m12s Loss: 0.6920 Dice: 0.4118\n",
      "[9/10][11/20] Elapsed_time: 0m13s Loss: 0.6903 Dice: 0.4934\n",
      "[9/10][12/20] Elapsed_time: 0m15s Loss: 0.6872 Dice: 0.4472\n",
      "[9/10][13/20] Elapsed_time: 0m16s Loss: 0.6903 Dice: 0.3789\n",
      "[9/10][14/20] Elapsed_time: 0m17s Loss: 0.6888 Dice: 0.3970\n",
      "[9/10][15/20] Elapsed_time: 0m18s Loss: 0.6913 Dice: 0.4455\n",
      "[9/10][16/20] Elapsed_time: 0m19s Loss: 0.6896 Dice: 0.5021\n",
      "[9/10][17/20] Elapsed_time: 0m20s Loss: 0.6904 Dice: 0.4240\n",
      "[9/10][18/20] Elapsed_time: 0m22s Loss: 0.6923 Dice: 0.4649\n",
      "[9/10][19/20] Elapsed_time: 0m23s Loss: 0.6892 Dice: 0.4708\n",
      "Elapsed_time: 0m1s Val dice: 0.5354\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "save_every=10\n",
    "all_error = np.zeros(0)\n",
    "all_error_L1 = np.zeros(0)\n",
    "all_error_dice = np.zeros(0)\n",
    "all_dice = np.zeros(0)\n",
    "all_val_dice = np.zeros(1)\n",
    "all_val_error = np.zeros(0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ##########\n",
    "    # Train\n",
    "    ##########\n",
    "    t0 = time.time()\n",
    "    for i, (data, label) in enumerate(train_loader):\n",
    "        \n",
    "        # setting your network to train will ensure that parameters will be updated during training, \n",
    "        # and that dropout will be used\n",
    "        net.train()\n",
    "        net.zero_grad()\n",
    "\n",
    "        target_real = torch.ones(data.size()[0])\n",
    "        batch_size = data.size()[0]\n",
    "        pred = net(data)\n",
    "        \n",
    "        # dice loss = 1-dice_coeff\n",
    "        # ----------------------------------------------- task 3 ------------------------------------------------------------\n",
    "        # Task 3: change loss function here\n",
    "        err = 1- dice_coeff(pred, label)\n",
    "        err = loss_BCE(pred, label)\n",
    "        # -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        dice_value = dice_coeff(pred, label).item()\n",
    "\n",
    "        err.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        time_elapsed = time.time() - t0\n",
    "        print('[{:d}/{:d}][{:d}/{:d}] Elapsed_time: {:.0f}m{:.0f}s Loss: {:.4f} Dice: {:.4f}'\n",
    "              .format(epoch, epochs, i, len(train_loader), time_elapsed // 60, time_elapsed % 60,\n",
    "                      err.item(), dice_value))\n",
    "\n",
    "        if i % save_every == 0:\n",
    "            # setting your network to eval mode to remove dropout during testing\n",
    "            net.eval()\n",
    "\n",
    "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_train_data.png' % (path, epoch, i),\n",
    "                                  normalize=True)\n",
    "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_train_label.png' % (path, epoch, i),\n",
    "                                  normalize=True)\n",
    "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_train_pred.png' % (path, epoch, i),\n",
    "                                  normalize=True)\n",
    "\n",
    "            error = err.item()\n",
    "\n",
    "            all_error = np.append(all_error, error)\n",
    "            all_dice = np.append(all_dice, dice_value)\n",
    "\n",
    "    # #############\n",
    "    # # Validation\n",
    "    # #############\n",
    "    mean_error = np.zeros(0)\n",
    "    mean_dice = np.zeros(0)\n",
    "    t0 = time.time()\n",
    "    for i, (data, label) in enumerate(val_loader):\n",
    "\n",
    "        net.eval()\n",
    "        batch_size = data.size()[0]\n",
    "\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        pred = net(data)\n",
    "        \n",
    "        # ----------------------------------------------- task 3 ------------------------------------------------------------\n",
    "        # Task 3: change loss function here\n",
    "        err = 1-dice_coeff(pred, label)\n",
    "        # err = loss_BCE(pred, label)\n",
    "        # -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # compare generated image to data-  metric\n",
    "        dice_value = dice_coeff(pred, label).item()\n",
    "\n",
    "        if i == 0:\n",
    "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_val_data.png' % (path, epoch, i),\n",
    "                              normalize=True)\n",
    "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_val_label.png' % (path, epoch, i),\n",
    "                              normalize=True)\n",
    "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_val_pred.png' % (path, epoch, i),\n",
    "                              normalize=True)\n",
    "\n",
    "        error = err.item()\n",
    "        mean_error = np.append(mean_error, error)\n",
    "        mean_dice = np.append(mean_dice, dice_value)\n",
    "\n",
    "    all_val_error = np.append(all_val_error, np.mean(mean_error))\n",
    "    all_val_dice = np.append(all_val_dice, np.mean(mean_dice))\n",
    "\n",
    "    time_elapsed = time.time() - t0\n",
    "\n",
    "    print('Elapsed_time: {:.0f}m{:.0f}s Val dice: {:.4f}'\n",
    "          .format(time_elapsed // 60, time_elapsed % 60, mean_dice.mean()))\n",
    "    \n",
    "    \n",
    "    num_it_per_epoch_train = ((train_loader.dataset.x_data.shape[0] * (1 - 0.2)) // (\n",
    "            save_every * batch_size)) + 1\n",
    "    epochs_train = np.arange(1,all_error.size+1) / num_it_per_epoch_train\n",
    "    epochs_val = np.arange(0,all_val_dice.size)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs_val, all_val_dice, label='dice_val')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend()\n",
    "    plt.title('Dice score')\n",
    "    plt.savefig(path + '/dice_val.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "the results are saved to a folder \\results - so please check that:\n",
    "\n",
    "The results are saved per epoch for both training and validation, and are saved as the \n",
    "1. real data, \n",
    "2. binary labels, \n",
    "3. predicted labels. \n",
    "\n",
    "In this example since we trained on a small sample of the data (100 crops) the results are far from optimal, and are likely to overfit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "1. Change the dice loss to a cross entropy loss in the code - is dice loss or cross entropy loss better?\n",
    "2. run the training with dropout - what's the effect?\n",
    "\n",
    "**Note down your dice validation scores for each experiment, then change**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
