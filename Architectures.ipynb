{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3, Notebook 2: CNN Architectures\n",
    "\n",
    "Tutorial by Cher Bass\n",
    "(edited by Emma Robinson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the modules and Data that we need for the notebook. We start by testing on the MNIST dataset as before. All other data you must download from this link https://emckclac-my.sharepoint.com/:f:/g/personal/k1776009_kcl_ac_uk/EqtcEv0DG4xDgemokhyFWlABQdA6mMbddZaIaRGGIV_SyA?e=0A9JZr. If you intend to use Google Colab for this tutorial, you must upload this data folder to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #contains some useful functions like activation functions & convolution operations you can use\n",
    "\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# This is used to transform the images to Tensor and normalize it\n",
    "transform = transforms.Compose(\n",
    "   [transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "training = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(training, batch_size=8,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "testing = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                      download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testing, batch_size=8,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "          '4', '5', '6', '7', '8', '9')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set your device to cuda (if you have access to a GPU) or cpu otherwise \n",
    "\n",
    "**hint** see lecture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet with pytorch\n",
    "\n",
    "ResNet was first introduced in 2016 as a way to deal with the gradient vanishing problem. This can occur when the network is too deep, and the gradients shrink to zero after a few back propagation steps. This can result in the parameter weights not being updated, since the gradient is zero.\n",
    "\n",
    "ResNets can counter this problem by allowing the gradients to flow directly backwards, by adding the additive resnet connections.\n",
    "\n",
    "An example of a resnet block (from the original 2016 paper) is illustrated below:\n",
    "\n",
    "![resnet-block](imgs/resnet-block.png)\n",
    "source: https://d2l.ai/chapter_convolutional-modern/resnet.html\n",
    "\n",
    "He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
    "\n",
    "http://www.pabloruizruiz10.com/resources/CNNs/ResNet-PyTorch.html\n",
    "\n",
    "https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8\n",
    "\n",
    "\n",
    "### Using existing ResNet \n",
    "\n",
    "It's possible to load existing networks using pytorch library torchvision - you can load these models using torchvision.models, which contains networks such as ResNet, Alexnet, VGG, Densenet, etc...\n",
    "https://pytorch.org/docs/stable/torchvision/models.html\n",
    "\n",
    "For example the following pretrained resnets models can be loaded in Pytorch:\n",
    "```python\n",
    "torchvision.models.resnet18(pretrained=True, **kwargs)\n",
    "```\n",
    "\n",
    "You can also load a model that hasn't been pretrained in the following way:\n",
    "```python\n",
    "torchvision.models.resnet18(pretrained=False, **kwargs)\n",
    "```\n",
    "\n",
    "You can find examples of how to use pretrained models in: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "\n",
    "However, you will find that using a pretrained model doesn't always suit your needs. For example, the resnet models shown above have been trained on RGB images (i.e. they are 3 channels), which means that you can't use them without adjustment on grayscale images, or on 3D medical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1 Programming your own ResNet\n",
    "\n",
    "The first thing we need to do is implement a `ResidualBlock` class, which will implement a single ResNet (2016) block, which includes the following steps: \n",
    "\n",
    "1. Convolution, followed by batchnorm, followed by relu\n",
    "2. Convolution, followed by batchnorm \n",
    "3. shortcut step, where \n",
    "    - the input is first transformed through a strided $1 \\times 1$ convolutional operation to match the dimensions of the output of the residual block\n",
    "    - added to the output of the convolutions. \n",
    "4. relu\n",
    "\n",
    "The only slightly challenging bit here is the first part of the shortcut step. So let's start by ignoring it to create the main body of the residual block. This will work provided we maintain input dimensions. \n",
    "\n",
    "#### To do 2.1.1 - Create the Residual  block\n",
    "\n",
    "As shown above the residual block performs `Conv2d > BatchNorm2d > ReLU > Conv2D BatchNorm2d > ADD > ReLU `. Let us create a `ResidualBlock` and define (parametrise) the required `Conv2d` and `BatchNorm2d` steps in the constructor (`__init__`):\n",
    "\n",
    "Tasks: edit (`__init__`) to input\n",
    "\n",
    "1. `self.conv1` a 2D convolution with arguments `in_channels=channels1,out_channels=channels2, kernel_size=3, stride=res_stride, padding=1, bias=False`. Here, `channels1, channels2 and res_stride` are input arguments to `__init()`. \n",
    "2. ` self.bn1` a 2D batchnorm layer with input `num_features=channels`\n",
    "2. `self.conv2` the second convolutional layer. *This time stride should be 1*. What should its input and output channel dimensions be? (note `kernel_size=3, stride=1, padding=1, bias=False` as before)\n",
    "4. ` self.bn1` the second 2D batchnorm layer. What does it expect for the number of input features (`num_features`)\n",
    "\n",
    "See:\n",
    "https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d\n",
    "https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d\n",
    "\n",
    "For PyTorch documentation for each of these functions, included argument variable names and expect input/output.\n",
    "\n",
    "Note, biases are set to `False` in the block as they are instead handled by the batchnorm layer see https://discuss.pytorch.org/t/why-does-the-resnet-model-given-by-pytorch-omit-biases-from-the-convolutional-layer/10990/2. Also, observe that the Relu layer is implemented in the forward pass function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channels1,channels2,res_stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.inplanes=channels1\n",
    "        # Exercise 2.1.1 construct the block without shortcut\n",
    "        self.conv1 = nn.Conv2d(channels1, channels2, kernel_size=3, \n",
    "                               stride=res_stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels2)\n",
    "        self.conv2 = nn.Conv2d(channels2, channels2, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels2)\n",
    "\n",
    "        if res_stride != 1 or channels2 != channels1:\n",
    "        # Exercise 2.1.3 the shortcut; create option for resizing input \n",
    "            self.shortcut=nn.Sequential(\n",
    "                nn.Conv2d(channels1, channels2, kernel_size=1, \n",
    "                          stride=res_stride, bias=False),\n",
    "                nn.BatchNorm2d(channels2)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut=nn.Sequential()\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # forward pass: Conv2d > BatchNorm2d > ReLU > \n",
    "        #Conv2D >  BatchNorm2d > ADD > ReLU\n",
    "        out=self.conv1(x)\n",
    "        out=self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        # THIS IS WHERE WE ADD THE INPUT\n",
    "        #print('input shape',x.shape,self.inplanes)\n",
    "        out += self.shortcut(x)\n",
    "       # print('res block output shape',  out.shape)\n",
    "        # final ReLu\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the steps of the forward pass. The networks performs the first convolution (followed by batchnorm and relu), then the second convolution (followed just by batchnorm). Then it adds the input. Here, the input is represented by the nn.Sequential() `self.shortcut` which we have currently left empty. Finally, the last operation of the block is a relu.\n",
    "\n",
    "#### To do 2.1.2 . Perform a test forward pass (keeping input and output dimensions constant\n",
    "\n",
    "1. Instantiate an instance of class ResidualBlock (create a network called `blk`)\n",
    "2. create a random tensor of size $5 \\times 3 \\times 100 \\times 100$ (which matches expected input dimensions $N,C_in,H,W$ the expected input dimensions of `nn.conv2d`\n",
    "3. Pass the input through a forward pass with number of input channels 3 and output channels 3\n",
    "\n",
    "**hint** look at how this was done in the last lecture. Remember - we don't need to explicitely call the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 100, 100]) torch.Size([5, 3, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "##  Student To do \n",
    "\n",
    "blk = ResidualBlock(3,3)\n",
    "X = random_ims = torch.randint(0, 255, (5,3,100,100)).to(torch.float)\n",
    "\n",
    "output=blk(X)\n",
    "print(X.shape,output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do 2.1.3. Implement the shortcut \n",
    "\n",
    "Now, let us go back and edit the function to support resizing the input. This will allow us to downsample and change the number of feature dimensions within our residual block.\n",
    "\n",
    "Change line 14 in `ResidualBlock.__init__()` to implement a Sequential block with two steps:\n",
    "1. A $1 \\times 1 $ `nn.Conv2d` layer with `stride=res_stride,bias=False`.  This will support changes of spatial dimensions through strided convolutions and changes of feature dimensions through $1 \\times 1 $ convolutions. What should your input and output channels be to make it equivalent to the output of the residual block?\n",
    "2. batchnorm. Think carefully about the input dimension. \n",
    "\n",
    "Once you have done this, test the network again, but this time change the number of output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 100, 100]) torch.Size([5, 10, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "blk = ResidualBlock(3,10,2)\n",
    "output=blk(X)\n",
    "print(X.shape,output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now have all the building blocks we need to build a residual network. In what follows we will construct a ResNet with four residual layers. Each layer will contain 2 residual blocks. \n",
    "\n",
    "#### To do 2.1.4 : Complete the Residual Network class\n",
    "\n",
    "**Step 1** Following the definition in the original paper the network starts with a  convolutional layer with a $7 \\times 7 $ kernel, followed by a batchnorm. However, as we intend to test on the MNIST (which is very small) lets change the $7 \\times 7 $ kernel to a $3 \\times 3 $ one (**check how this is implemented**) \n",
    "\n",
    "**Step 2 (Student complete)** Comment the function `_make_layer`. What is each line doing? Complete the class constructor, using `_make_layer` to create 4 residual layers,  with `num_blocks` residual blocks per layer, `num_strides[i]` strides per block (where $i$ indexes the layer, starting from the initial convolution) and `num_features[i]` represents the number of output channels per layer.\n",
    "\n",
    "**Step 4 (Student complete)** The last layer is a fully connected (softmax) layer. Complete this function. The number of inputs must match the number of outputs from the previous layer and the number of outputs must match the number of classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_strides, num_features, in_channels, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # step 1. Initialising the network with a 3 x3 conv and batch norm\n",
    "        self.conv1 = nn.Conv2d(in_channels, num_features[0], kernel_size=3, \n",
    "                               stride=num_strides[0], padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # Step 2: TO DO Using function make_layer() create 4 residual layers\n",
    "        # num_blocks per layer is given by input argument num_blocks (which is an array)\n",
    "        self.layer1 = self._make_layer(block, num_features[1], num_blocks, stride=num_strides[1])\n",
    "        self.layer2 = self._make_layer(block, num_features[2], num_blocks, stride=num_strides[2])\n",
    "        self.layer3 = self._make_layer(block, num_features[3], num_blocks, stride=num_strides[3])\n",
    "        self.layer4 = self._make_layer(block, num_features[4], num_blocks, stride=num_strides[4])\n",
    "        self.linear = nn.Linear(num_features[4], num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        layers = []\n",
    "        \n",
    "        for i in np.arange(num_blocks -1):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "            self.in_planes = planes \n",
    "        \n",
    "        layers.append(block(planes, planes, stride))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        print('init',out.shape)\n",
    "        out = self.layer1(out)\n",
    "        print('layer1',out.shape)\n",
    "        out = self.layer2(out)\n",
    "        print('layer2',out.shape)\n",
    "\n",
    "        out = self.layer3(out)\n",
    "        print('layer3',out.shape)\n",
    "\n",
    "        out = self.layer4(out)\n",
    "        print('layer4',out.shape)\n",
    "\n",
    "        \n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        print('flat',out.shape)\n",
    "        out = self.linear(out)\n",
    "        print('linear',out.shape)\n",
    "        return F.log_softmax(out,dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5** Observe below, creation of an instance of class `ResNet`. This requires as argument the `ResidualBlock` class defined above. We hard code the argments for number of blocks, as well as lists defining the number of strides and features per layer. These lists have length 5 to encode also for the initial convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_ResNet4(in_channels=1):\n",
    "    return ResNet(ResidualBlock,2, [1,1,2,2,2], [64,64,128,256,512], in_channels=in_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do 2.1.5:  run your ResNet on MNIST for classification\n",
    "Create a ResNet network and run with the same code as above for classification, and then test.\n",
    "Remember to define your loss function, optimizer, dataloaders, and your resnet network. \n",
    "Then run the training and testing, as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------task 4 -----------------------------------------------------\n",
    "# Task 4: Train and test ResNet on MNIST dataset for classification\n",
    "# hints: define your resnet network, loss function, optimizer and dataloaders. \n",
    "# Then you can run the same training and testing code as above.\n",
    "# ----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "resnet = my_ResNet4(in_channels=1)\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init torch.Size([8, 64, 28, 28])\n",
      "layer1 torch.Size([8, 64, 28, 28])\n",
      "layer2 torch.Size([8, 128, 14, 14])\n",
      "layer3 torch.Size([8, 256, 7, 7])\n",
      "layer4 torch.Size([8, 512, 4, 4])\n",
      "torch.Size([8, 512, 1, 1])\n",
      "flat torch.Size([8, 512])\n",
      "linear torch.Size([8, 10])\n",
      "[1,     1] loss: 2.484\n",
      "init torch.Size([8, 64, 28, 28])\n",
      "layer1 torch.Size([8, 64, 28, 28])\n",
      "layer2 torch.Size([8, 128, 14, 14])\n",
      "layer3 torch.Size([8, 256, 7, 7])\n",
      "layer4 torch.Size([8, 512, 4, 4])\n",
      "torch.Size([8, 512, 1, 1])\n",
      "flat torch.Size([8, 512])\n",
      "linear torch.Size([8, 10])\n",
      "init torch.Size([8, 64, 28, 28])\n",
      "layer1 torch.Size([8, 64, 28, 28])\n",
      "layer2 torch.Size([8, 128, 14, 14])\n",
      "layer3 torch.Size([8, 256, 7, 7])\n",
      "layer4 torch.Size([8, 512, 4, 4])\n",
      "torch.Size([8, 512, 1, 1])\n",
      "flat torch.Size([8, 512])\n",
      "linear torch.Size([8, 10])\n",
      "init torch.Size([8, 64, 28, 28])\n",
      "layer1 torch.Size([8, 64, 28, 28])\n",
      "layer2 torch.Size([8, 128, 14, 14])\n",
      "layer3 torch.Size([8, 256, 7, 7])\n",
      "layer4 torch.Size([8, 512, 4, 4])\n",
      "torch.Size([8, 512, 1, 1])\n",
      "flat torch.Size([8, 512])\n",
      "linear torch.Size([8, 10])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e771ee2e29fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#compute the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs): \n",
    "\n",
    "    # enumerate can be used to output iteration index i, as well as the data \n",
    "    for i, (data, labels) in enumerate(train_loader, 0):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #feed the input and acquire the output from network\n",
    "        outputs = resnet(data)\n",
    "\n",
    "        #calculating the predicted and the expected loss\n",
    "        loss = loss_fun(outputs, labels)\n",
    "\n",
    "        #compute the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        #update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        ce_loss = loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                 (epoch + 1, i + 1, ce_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#make an iterator from test_loader\n",
    "#Get a batch of testing images\n",
    "test_iterator = iter(test_loader)\n",
    "images, labels = test_iterator.next()\n",
    "\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "y_score = resnet(images)\n",
    "# get predicted class from the class probabilities\n",
    "_, y_pred = torch.max(y_score, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[y_pred[j]] for j in range(8)))\n",
    "rows = 2\n",
    "columns = 4\n",
    "# plot y_score - true label (t) vs predicted label (p)\n",
    "fig2 = plt.figure()\n",
    "for i in range(8):\n",
    "    fig2.add_subplot(rows, columns, i+1)\n",
    "    plt.title('t: ' + classes[labels[i].cpu()] + ' p: ' + classes[y_pred[i].cpu()])\n",
    "    img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
    "    img = torchvision.transforms.ToPILImage()(img.cpu())\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = labels.data.cpu().numpy()\n",
    "y_pred = y_pred.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "print('accuracy:', accuracy, ', f1 score:', f1, ', precision:', precision, ', recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional Exercise) Use ResNet for classification - CIFAR10\n",
    "\n",
    "Use the torch inbuilt ResNet for RBG images and train for classification on the CIFAR10 dataset.\n",
    "\n",
    "Here are some example images from the CIFAR10 datasets- we have 10 classes:\n",
    "\n",
    "![cifar10](imgs/cifar10.jpg)\n",
    "source: https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\n",
    "\n",
    "You can load the CIFAR10 dataset using torchvision in the following way:\n",
    "```python\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=8,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "```\n",
    "You can use this tutorial as a reference for training on CIFAR10 - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "Remember to define your loss function, optimizer, dataloaders, and your resnet network. \n",
    "Then run the training and testing, same as with MNIST.\n",
    "\n",
    "First, import the PyTorch ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnet_cifar = models.resnet18(pretrained=True)\n",
    "resnet_cifar = resnet_cifar.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------task 5 -----------------------------------------------------\n",
    "# Task 5: Train and test ResNet on CIFAR10 dataset for classification\n",
    "# hints: define your resnet network, loss function, optimizer and dataloaders. \n",
    "# Then you can run the same training and testing code as above.\n",
    "# ----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=8,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#resnet_cifar = ResNet18(in_channels=3)\n",
    "#resnet_cifar = resnet_cifar.to(device)\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "loss_fun = loss_fun.to(device)\n",
    "\n",
    "optimizer = optim.SGD(resnet_cifar.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs): \n",
    "\n",
    "    # enumerate can be used to output iteration index i, as well as the data \n",
    "    for i, (data, labels) in enumerate(train_loader, 0):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #feed the input and acquire the output from network\n",
    "        outputs = resnet_cifar(data)\n",
    "\n",
    "        #calculating the predicted and the expected loss\n",
    "        loss = loss_fun(outputs, labels)\n",
    "\n",
    "        #compute the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        #update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        ce_loss = loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                 (epoch + 1, i + 1, ce_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an iterator from test_loader\n",
    "#Get a batch of testing images\n",
    "test_iterator = iter(test_loader)\n",
    "images, labels = test_iterator.next()\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "y_score = resnet_cifar(images)\n",
    "# get predicted class from the class probabilities\n",
    "_, y_pred = torch.max(y_score, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[y_pred[j]] for j in range(8)))\n",
    "\n",
    "# plot y_score - true label (t) vs predicted label (p)\n",
    "fig2 = plt.figure()\n",
    "for i in range(8):\n",
    "    fig2.add_subplot(rows, columns, i+1)\n",
    "    plt.title('t: ' + classes[labels[i].cpu()] + ' p: ' + classes[y_pred[i].cpu()])\n",
    "    img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
    "    img = torchvision.transforms.ToPILImage()(img.cpu())\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = labels.data.cpu().numpy()\n",
    "y_pred = y_pred.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "print('accuracy:', accuracy, ', f1 score:', f1, ', precision:', precision, ', recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2 Image segmentation with pytorch using U-net\n",
    "\n",
    "U-net was first developed in 2015 by Ronneberger et al., as a segmentation network for biomedical image analysis.\n",
    "It has been extremely successful, with 9,000+ citations, and many new methods that have used the U-net architecture since.\n",
    "\n",
    "\n",
    "The architecture of U-net is based on the idea of using skip connections (i.e. concatenating) at different levels of the network to retain high, and low level features.\n",
    "\n",
    "Here is the architecture of a U-net:\n",
    "\n",
    "---\n",
    "\n",
    "![U-net](imgs/unet.png)\n",
    "Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-photon microscopy dataset of cortical axons\n",
    "\n",
    "In this tutorial we use a dataset of cortical neurons with their corresponding segmentation binary labels.\n",
    "\n",
    "These images were collected using in-vivo two-photon microscopy from the mouse somatosensory cortex. To generate the 2D images, a max projection was used over the 3D stack. The labels are binary segmentation maps of the axons.\n",
    "\n",
    "Here we will use 100 [64x64] crops during training and validation. \n",
    "\n",
    "These are some example images [256x256] from the original dataset:\n",
    "![axon_dataset](imgs/axon_dataset.png)\n",
    "\n",
    "Bass, Cher, et al. \"Image synthesis with a convolutional capsule generative adversarial network.\" Medical Imaging with Deep Learning (2019).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emma/King's College London/Cardoso, Jorge - Advanced_Machine_Learning/03_Loss_functions_and_Optimisers/Lecture-3-Data\n"
     ]
    }
   ],
   "source": [
    "%cd ../Lecture-3-Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load modules\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from AxonDataset import AxonDataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emma/King's College London/Cardoso, Jorge - Advanced_Machine_Learning/03_Loss_functions_and_Optimisers/Lecture-3-Data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setting parameters\n",
    "timestr = time.strftime(\"%d%m%Y-%H%M\")\n",
    "__location__ = os.path.realpath(\n",
    "    os.path.join(os.getcwd(), os.path.dirname('__file__')))\n",
    "\n",
    "print(__location__)\n",
    "\n",
    "path = os.path.join(__location__,'results')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "# Define your batch_size\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataloader\n",
    "\n",
    "In this example, a custom dataloader was created, and we import it from `AxonDataset.py`\n",
    "\n",
    "We utilise the `torch.utils.data.sampler.SubsetRandomSampler` to create two DataLoaders for train and validation. Here, a random a subset of 20% of subject indices are selected for validation. The remaining 80% are used for training. The lists of train and validation subjects are passed to `torch.utils.data.sampler.SubsetRandomSampler` to create bespoke train/validation samplers; these are passed to the `DataLoader` using the argument `sampler,` and override the default use of `shuffle`.\n",
    "\n",
    "#### 2.2.1 Create a list of random indices for train and validation sets (**hint** use np.random.choice)\n",
    "\n",
    "Check you understand how the bespoke samplers are implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we create a dataloader for our example dataset- two photon microscopy with axons\n",
    "axon_dataset = AxonDataset(data_name='org64', type='train')\n",
    "\n",
    "# -----------------------------------------------------task 1----------------------------------------------------------------\n",
    "# Task 1: create a random list of indices for training and testing with a 80%,20% split\n",
    "\n",
    "# We need to further split our training dataset into training and validation sets.\n",
    "# Define the indices\n",
    "indices = list(range(len(axon_dataset)))  # start with all the indices in training set\n",
    "split = int(len(indices)*0.2)  # define the split size\n",
    "\n",
    "# Get indices for train and validation datasets, and split the data\n",
    "validation_idx = np.random.choice(indices, size=split, replace=False)\n",
    "train_idx = list(set(indices) - set(validation_idx))\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# feed indices into the sampler\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "\n",
    "# Create a dataloader instance \n",
    "train_loader = torch.utils.data.DataLoader(axon_dataset, batch_size = batch_size,\n",
    "                                           sampler=train_sampler) \n",
    "val_loader = torch.utils.data.DataLoader(axon_dataset, batch_size = batch_size,\n",
    "                                        sampler=validation_sampler) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a U-net \n",
    "\n",
    "We next build our u-net network.\n",
    "\n",
    "First we define a layer `double_conv` that performs 2 sets of convolution followed by ReLu.This is set up as a `nn.Sequential(` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define U-net\n",
    "def double_conv(in_channels, out_channels, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=padding),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define how we perform an downsample and an upsample step. The original U-net performs downsampling through a $2 \\times 2 $ max pool (however, strided convolutions are equally viable). Upsampling is performed through use of `nn.Upsample` (https://pytorch.org/docs/stable/nn.html#torch.nn.Upsample), which interpolates the data to a higher resolution grid. The function expects arguments `scale_factor` and (interpolation) `mode`. There are several options for the interpolation mode; we recommend bilinear. In this example we upsample by a `scale_factor` of 2 each time (to match the $2\\times 2$ max pool used during downsampling). \n",
    "\n",
    "Thus, in what follows, a single level of encoding can be represented as:\n",
    "\n",
    "`conv1 = self.dconv_down1(x)\n",
    " conv1 = self.dropout(conv1)\n",
    " x = self.maxpool(conv1)`\n",
    "        \n",
    "In other words a double convolution followed by a maxpool. Here, a dropout layer is inserted between the convolutional layer and the maxpool for regularisation. An alternative approach is to insert a batchnorm between the `nn.Conv2d` and the `nn.ReLU` e.g. https://github.com/milesial/Pytorch-UNet\n",
    "\n",
    "A single level of decoding might be represented as:\n",
    "\n",
    "`deconv4 = self.upsample(conv5)\n",
    " deconv4  = self.dconv_up4(deconv4)\n",
    " deconv4 = self.dropout(deconv4)`\n",
    " \n",
    "However, we are missing something vital...\n",
    "\n",
    "### Skip connections\n",
    "\n",
    "The U-net is a symmetric network with equal numbers of encoding and decoding layers. These form pairs where the spatial dimensions of each encoder/decoder layer in the pair are consistent.\n",
    "\n",
    "A key feature of the U-net is that to support segmentation of sharp boundaries with preservation of high spatial resolution features it is necessary to pass features learnt during encoding across the network. The theory is that the early layers, with their small-receptive fields, learn the high-spatial frequency information (i.e. they act as edge detectors and/or texture filters). As the receptive field increases during encoding spatial specicity is lost, but spatial localisation (where class relevant objects broadly are in the image) is gained. In order to import the high spatial frequency information of the early encoding layers into the final decoding layers the *activations* learnt during encoding are directly concatenated onto the upsampled activations of the paired decoding layer.\n",
    "\n",
    "In other words for the first decoding layer (which for a 5-layer U-Net is the layer that directly follows the bottleneck `conv5`) is:\n",
    "\n",
    "`deconv4 = self.upsample(conv5)\n",
    " deconv4 = torch.cat([deconv4, conv4], dim=1)\n",
    " deconv4  = self.dconv_up4(deconv4)\n",
    " deconv4 = self.dropout(deconv4)`\n",
    " \n",
    " The activations (output) of convolution layer conv (`conv4`) is directly concatenated to the output of `self.upsample` where concatenation is performed on the channel axis (`axis=1`); Thus putting this all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a U-Net\n",
    "\n",
    "We then define our U-net network.\n",
    "\n",
    "We first initialise all the different layers in the network in `__init__`:\n",
    "1. `self.dconv_down1` is a double convolutional layer (defined above)\n",
    "2. `self.maxpool` is a max pooling layer that is used to reduce the size of the input, and increase the receptive field\n",
    "3. `self.upsample` is an upsampling layer that is used to increase the size of the input\n",
    "4. `dropout` is a dropout layer that is applied to regularise the training\n",
    "5. `dconv_up4` is also a double convolutional layer- note that it takes in additional channels from previous layers (i.e. the skip connections).\n",
    "\n",
    "\n",
    "### To do 2.2.2  complete the forward pass\n",
    "\n",
    "1. Following the example for conv1 complete encoder layers 2,3 and 4. How many features does each layer have?\n",
    "2. Complete layer `conv5`; this is the bottleneck layer (the bottom of the network) and thus has no maxpool.\n",
    "2. Using the upsampling and skip connection example above implement the decoder layers `deconv4`,`deconv3`,`deconv2`,`deconv1`.\n",
    "5. We are expecting class labels as output; thus the output requires a sigmoid transformation; check you understand what this does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dconv_down1 = double_conv(1, 32)\n",
    "        self.dconv_down2 = double_conv(32, 64)\n",
    "        self.dconv_down3 = double_conv(64, 128)\n",
    "        self.dconv_down4 = double_conv(128, 256)\n",
    "        self.dconv_down5 = double_conv(256, 512)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.dconv_up4 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up3 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up2 = double_conv(128 + 64, 64)\n",
    "        self.dconv_up1 = double_conv(64 + 32, 32)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(32, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #######   ENCODER ###############\n",
    "        \n",
    "        conv1 = self.dconv_down1(x)\n",
    "        conv1 = self.dropout(conv1)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        # --------------------------------------------------- task 2.2.1 ----------------------------------------------------------\n",
    "        # implement encoder layers conv2, conv3 and conv4\n",
    "        \n",
    "        conv2 = self.dconv_down2(x)\n",
    "        conv2 = self.dropout(conv2)\n",
    "        x = self.maxpool(conv2)\n",
    "\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        conv3 = self.dropout(conv3)\n",
    "        x = self.maxpool(conv3)\n",
    "\n",
    "        conv4 = self.dconv_down4(x)\n",
    "        conv4 = self.dropout(conv4)\n",
    "        x = self.maxpool(conv4)\n",
    "\n",
    "        # --------------------------------------------------- task 2.2.2 ----------------------------------------------------------\n",
    "        # implement bottleneck\n",
    "        \n",
    "        conv5 = self.dconv_down5(x)\n",
    "        conv5 = self.dropout(conv5)\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "       \n",
    "        #######   DECODER ###############\n",
    "        \n",
    "        # --------------------------------------------------- task 2.2.3 ----------------------------------------------------------\n",
    "        # Implement the decoding layers\n",
    "        \n",
    "        deconv4 = self.upsample(conv5)\n",
    "        deconv4 = torch.cat([deconv4, conv4], dim=1)  \n",
    "        deconv4  = self.dconv_up4(deconv4)\n",
    "        deconv4 = self.dropout(deconv4)\n",
    "\n",
    "        deconv3 = self.upsample(deconv4 )       \n",
    "        deconv3 = torch.cat([deconv3, conv3], dim=1)\n",
    "        deconv3 = self.dconv_up3(deconv3)\n",
    "        deconv3 = self.dropout(deconv3)\n",
    "\n",
    "        deconv2 = self.upsample(deconv3)      \n",
    "        deconv2 = torch.cat([deconv2, conv2], dim=1)\n",
    "        deconv2 = self.dconv_up2(deconv2)\n",
    "        deconv2 = self.dropout(deconv2)\n",
    "       \n",
    "        deconv1 = self.upsample(deconv2)   \n",
    "        deconv1 = torch.cat([deconv1, conv1], dim=1)\n",
    "        deconv1 = self.dconv_up1(deconv1)\n",
    "        deconv1 = self.dropout(deconv1)\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------------------\n",
    "        out = F.sigmoid(self.conv_last(deconv1))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time we initialise the network with a previously trained network by loading the weights\n",
    "\n",
    "*for practical reasons training this network from scratch will take too long, and require large computational resources*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise network - and load weights\n",
    "net = UNet()\n",
    "#net.load_state_dict(torch.load(path+'/'+'model.pt')) #this function loads a pretrained network\n",
    "net.load_state_dict(torch.load(path+'/'+'model.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "#adding line to support GPU use (where available)\n",
    "net=net.to_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an appropriate loss function\n",
    "We next define our loss function - in this case we use Dice loss, a commonly used loss for image segmentation.\n",
    "\n",
    "The Dice coefficient can be used as a loss function, and is essentially a measure of overlap between two samples.\n",
    "\n",
    "Dice is in the range of 0 to 1, where a Dice coefficient of 1 denotes perfect and complete overlap. The Dice coefficient was originally developed for binary data, and can be calculated as:\n",
    "\n",
    "$Dice = \\dfrac{2|A\\cap B|}{|A| + |B|}$\n",
    "\n",
    "where $|A\\cap B|$ represents the common elements between sets $A$ and $B$, and $|A|$ represents the number of elements in set $A$ (and likewise for set $B$).\n",
    "\n",
    "For the case of evaluating a Dice coefficient on predicted segmentation masks, we can approximate  $|A\\cap B|$ as the element-wise multiplication between the prediction and target mask, and then sum the resulting matrix.\n",
    "\n",
    "An **alternative loss** function would be pixel-wise cross entropy loss. It would examine each pixel individually, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice loss\n",
    "def dice_coeff(pred, target):\n",
    "    \"\"\"This definition generalize to real valued pred and target vector.\n",
    "    This should be differentiable.\n",
    "    pred: tensor with first dimension as batch\n",
    "    target: tensor with first dimension as batch\n",
    "    \"\"\"\n",
    "\n",
    "    smooth = 1.\n",
    "    epsilon = 10e-8\n",
    "\n",
    "    # have to use contiguous since they may from a torch.view op\n",
    "    iflat = pred.contiguous().view(-1)\n",
    "    tflat = target.contiguous().view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    A_sum = torch.sum(iflat * iflat)\n",
    "    B_sum = torch.sum(tflat * tflat)\n",
    "\n",
    "    dice = (2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "    dice = dice.mean(dim=0)\n",
    "    dice = torch.clamp(dice, 0, 1.0-epsilon)\n",
    "\n",
    "    return  dice\n",
    "\n",
    "# cross entropy loss\n",
    "loss_BCE = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the penalty term `smooth` is added to prevent division by zero.\n",
    "\n",
    "As before, we define the optimiser to train our network - here we use Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define your optimiser\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=1e-05, betas=(0.5, 0.999))\n",
    "optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating our segmentation network\n",
    "We next train and evaluate our network \n",
    "\n",
    "note that the results are saved to a folder \\results - so please check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][0/20] Elapsed_time: 0m2s Loss: 0.6962 Dice: 0.6620\n",
      "[0/10][1/20] Elapsed_time: 0m4s Loss: 0.7027 Dice: 0.6545\n",
      "[0/10][2/20] Elapsed_time: 0m5s Loss: 0.6957 Dice: 0.6479\n",
      "[0/10][3/20] Elapsed_time: 0m7s Loss: 0.6937 Dice: 0.5084\n",
      "[0/10][4/20] Elapsed_time: 0m8s Loss: 0.6928 Dice: 0.6425\n",
      "[0/10][5/20] Elapsed_time: 0m11s Loss: 0.6917 Dice: 0.6186\n",
      "[0/10][6/20] Elapsed_time: 0m13s Loss: 0.6936 Dice: 0.5032\n",
      "[0/10][7/20] Elapsed_time: 0m15s Loss: 0.6941 Dice: 0.3869\n",
      "[0/10][8/20] Elapsed_time: 0m17s Loss: 0.6931 Dice: 0.5092\n",
      "[0/10][9/20] Elapsed_time: 0m20s Loss: 0.6901 Dice: 0.4425\n",
      "[0/10][10/20] Elapsed_time: 0m23s Loss: 0.6933 Dice: 0.4190\n",
      "[0/10][11/20] Elapsed_time: 0m26s Loss: 0.6908 Dice: 0.4331\n",
      "[0/10][12/20] Elapsed_time: 0m29s Loss: 0.6906 Dice: 0.4859\n",
      "[0/10][13/20] Elapsed_time: 0m32s Loss: 0.6934 Dice: 0.3216\n",
      "[0/10][14/20] Elapsed_time: 0m36s Loss: 0.6905 Dice: 0.4074\n",
      "[0/10][15/20] Elapsed_time: 0m38s Loss: 0.6943 Dice: 0.3305\n",
      "[0/10][16/20] Elapsed_time: 0m44s Loss: 0.6900 Dice: 0.4501\n",
      "[0/10][17/20] Elapsed_time: 0m46s Loss: 0.6915 Dice: 0.4277\n",
      "[0/10][18/20] Elapsed_time: 0m50s Loss: 0.6923 Dice: 0.3909\n",
      "[0/10][19/20] Elapsed_time: 0m53s Loss: 0.6903 Dice: 0.4043\n",
      "Elapsed_time: 0m3s Val dice: 0.4579\n",
      "[1/10][0/20] Elapsed_time: 0m5s Loss: 0.6917 Dice: 0.2950\n",
      "[1/10][1/20] Elapsed_time: 0m9s Loss: 0.6906 Dice: 0.3676\n",
      "[1/10][2/20] Elapsed_time: 0m12s Loss: 0.6902 Dice: 0.3937\n",
      "[1/10][3/20] Elapsed_time: 0m15s Loss: 0.6915 Dice: 0.2933\n",
      "[1/10][4/20] Elapsed_time: 0m21s Loss: 0.6910 Dice: 0.4062\n",
      "[1/10][5/20] Elapsed_time: 0m24s Loss: 0.6913 Dice: 0.4045\n",
      "[1/10][6/20] Elapsed_time: 0m28s Loss: 0.6917 Dice: 0.3636\n",
      "[1/10][7/20] Elapsed_time: 0m31s Loss: 0.6916 Dice: 0.4414\n",
      "[1/10][8/20] Elapsed_time: 0m35s Loss: 0.6913 Dice: 0.3359\n",
      "[1/10][9/20] Elapsed_time: 0m38s Loss: 0.6914 Dice: 0.4563\n",
      "[1/10][10/20] Elapsed_time: 0m42s Loss: 0.7102 Dice: 0.2818\n",
      "[1/10][11/20] Elapsed_time: 0m45s Loss: 0.6917 Dice: 0.4481\n",
      "[1/10][12/20] Elapsed_time: 0m47s Loss: 0.6929 Dice: 0.4893\n",
      "[1/10][13/20] Elapsed_time: 0m51s Loss: 0.6924 Dice: 0.3381\n",
      "[1/10][14/20] Elapsed_time: 0m54s Loss: 0.6934 Dice: 0.2504\n",
      "[1/10][15/20] Elapsed_time: 0m58s Loss: 0.6912 Dice: 0.4071\n",
      "[1/10][16/20] Elapsed_time: 1m3s Loss: 0.6915 Dice: 0.3966\n",
      "[1/10][17/20] Elapsed_time: 1m6s Loss: 0.6920 Dice: 0.3769\n",
      "[1/10][18/20] Elapsed_time: 1m10s Loss: 0.6921 Dice: 0.3072\n",
      "[1/10][19/20] Elapsed_time: 1m15s Loss: 0.6911 Dice: 0.3875\n",
      "Elapsed_time: 0m3s Val dice: 0.4636\n",
      "[2/10][0/20] Elapsed_time: 0m4s Loss: 0.6914 Dice: 0.2796\n",
      "[2/10][1/20] Elapsed_time: 0m8s Loss: 0.6931 Dice: 0.2955\n",
      "[2/10][2/20] Elapsed_time: 0m11s Loss: 0.6907 Dice: 0.4045\n",
      "[2/10][3/20] Elapsed_time: 0m15s Loss: 0.6931 Dice: 0.2459\n",
      "[2/10][4/20] Elapsed_time: 0m18s Loss: 0.6916 Dice: 0.3129\n",
      "[2/10][5/20] Elapsed_time: 0m22s Loss: 0.6915 Dice: 0.3199\n",
      "[2/10][6/20] Elapsed_time: 0m25s Loss: 0.6913 Dice: 0.4364\n",
      "[2/10][7/20] Elapsed_time: 0m28s Loss: 0.6934 Dice: 0.3836\n",
      "[2/10][8/20] Elapsed_time: 0m31s Loss: 0.6898 Dice: 0.4388\n",
      "[2/10][9/20] Elapsed_time: 0m34s Loss: 0.6917 Dice: 0.2938\n",
      "[2/10][10/20] Elapsed_time: 0m37s Loss: 0.6907 Dice: 0.3597\n",
      "[2/10][11/20] Elapsed_time: 0m41s Loss: 0.6912 Dice: 0.3806\n",
      "[2/10][12/20] Elapsed_time: 0m43s Loss: 0.6932 Dice: 0.3073\n",
      "[2/10][13/20] Elapsed_time: 0m46s Loss: 0.6922 Dice: 0.4171\n",
      "[2/10][14/20] Elapsed_time: 0m48s Loss: 0.6898 Dice: 0.3985\n",
      "[2/10][15/20] Elapsed_time: 0m53s Loss: 0.6913 Dice: 0.3592\n",
      "[2/10][16/20] Elapsed_time: 0m56s Loss: 0.6901 Dice: 0.4452\n",
      "[2/10][17/20] Elapsed_time: 0m59s Loss: 0.6907 Dice: 0.3278\n",
      "[2/10][18/20] Elapsed_time: 1m2s Loss: 0.6912 Dice: 0.3805\n",
      "[2/10][19/20] Elapsed_time: 1m5s Loss: 0.6904 Dice: 0.4438\n",
      "Elapsed_time: 0m3s Val dice: 0.5291\n",
      "[3/10][0/20] Elapsed_time: 0m3s Loss: 0.6940 Dice: 0.3448\n",
      "[3/10][1/20] Elapsed_time: 0m7s Loss: 0.6923 Dice: 0.3487\n",
      "[3/10][2/20] Elapsed_time: 0m10s Loss: 0.6934 Dice: 0.3235\n",
      "[3/10][3/20] Elapsed_time: 0m13s Loss: 0.6927 Dice: 0.2926\n",
      "[3/10][4/20] Elapsed_time: 0m16s Loss: 0.6878 Dice: 0.4842\n",
      "[3/10][5/20] Elapsed_time: 0m19s Loss: 0.6897 Dice: 0.4468\n",
      "[3/10][6/20] Elapsed_time: 0m23s Loss: 0.6913 Dice: 0.4034\n",
      "[3/10][7/20] Elapsed_time: 0m27s Loss: 0.6901 Dice: 0.3617\n",
      "[3/10][8/20] Elapsed_time: 0m30s Loss: 0.6919 Dice: 0.4202\n",
      "[3/10][9/20] Elapsed_time: 0m33s Loss: 0.6903 Dice: 0.3650\n",
      "[3/10][10/20] Elapsed_time: 0m36s Loss: 0.6923 Dice: 0.3881\n",
      "[3/10][11/20] Elapsed_time: 0m39s Loss: 0.6893 Dice: 0.3886\n",
      "[3/10][12/20] Elapsed_time: 0m42s Loss: 0.6912 Dice: 0.4501\n",
      "[3/10][13/20] Elapsed_time: 0m45s Loss: 0.6932 Dice: 0.2986\n",
      "[3/10][14/20] Elapsed_time: 0m48s Loss: 0.6890 Dice: 0.4595\n",
      "[3/10][15/20] Elapsed_time: 0m51s Loss: 0.6911 Dice: 0.4417\n",
      "[3/10][16/20] Elapsed_time: 0m54s Loss: 0.6914 Dice: 0.3052\n",
      "[3/10][17/20] Elapsed_time: 0m58s Loss: 0.6906 Dice: 0.4599\n",
      "[3/10][18/20] Elapsed_time: 1m1s Loss: 0.6918 Dice: 0.4174\n",
      "[3/10][19/20] Elapsed_time: 1m4s Loss: 0.6892 Dice: 0.5264\n",
      "Elapsed_time: 0m4s Val dice: 0.5269\n",
      "[4/10][0/20] Elapsed_time: 0m4s Loss: 0.6923 Dice: 0.4154\n",
      "[4/10][1/20] Elapsed_time: 0m7s Loss: 0.6899 Dice: 0.4231\n",
      "[4/10][2/20] Elapsed_time: 0m10s Loss: 0.6916 Dice: 0.3468\n",
      "[4/10][3/20] Elapsed_time: 0m14s Loss: 0.6905 Dice: 0.3634\n",
      "[4/10][4/20] Elapsed_time: 0m17s Loss: 0.6912 Dice: 0.4140\n",
      "[4/10][5/20] Elapsed_time: 0m20s Loss: 0.6905 Dice: 0.3489\n",
      "[4/10][6/20] Elapsed_time: 0m24s Loss: 0.6899 Dice: 0.5000\n",
      "[4/10][7/20] Elapsed_time: 0m27s Loss: 0.6921 Dice: 0.4101\n",
      "[4/10][8/20] Elapsed_time: 0m31s Loss: 0.6903 Dice: 0.3550\n",
      "[4/10][9/20] Elapsed_time: 0m34s Loss: 0.6915 Dice: 0.4468\n",
      "[4/10][10/20] Elapsed_time: 0m37s Loss: 0.6913 Dice: 0.3591\n",
      "[4/10][11/20] Elapsed_time: 0m41s Loss: 0.6899 Dice: 0.3797\n",
      "[4/10][12/20] Elapsed_time: 0m44s Loss: 0.6915 Dice: 0.2901\n",
      "[4/10][13/20] Elapsed_time: 0m48s Loss: 0.6885 Dice: 0.4453\n",
      "[4/10][14/20] Elapsed_time: 0m52s Loss: 0.6901 Dice: 0.5250\n",
      "[4/10][15/20] Elapsed_time: 0m55s Loss: 0.6915 Dice: 0.4603\n",
      "[4/10][16/20] Elapsed_time: 0m59s Loss: 0.6923 Dice: 0.4208\n",
      "[4/10][17/20] Elapsed_time: 1m2s Loss: 0.6913 Dice: 0.3745\n",
      "[4/10][18/20] Elapsed_time: 1m6s Loss: 0.6904 Dice: 0.4198\n",
      "[4/10][19/20] Elapsed_time: 1m10s Loss: 0.6921 Dice: 0.3415\n",
      "Elapsed_time: 0m3s Val dice: 0.5192\n",
      "[5/10][0/20] Elapsed_time: 0m3s Loss: 0.6910 Dice: 0.3137\n",
      "[5/10][1/20] Elapsed_time: 0m5s Loss: 0.6917 Dice: 0.3548\n",
      "[5/10][2/20] Elapsed_time: 0m9s Loss: 0.6907 Dice: 0.4211\n",
      "[5/10][3/20] Elapsed_time: 0m12s Loss: 0.6911 Dice: 0.4744\n",
      "[5/10][4/20] Elapsed_time: 0m16s Loss: 0.6913 Dice: 0.4136\n",
      "[5/10][5/20] Elapsed_time: 0m19s Loss: 0.6917 Dice: 0.4310\n",
      "[5/10][6/20] Elapsed_time: 0m23s Loss: 0.6904 Dice: 0.3464\n",
      "[5/10][7/20] Elapsed_time: 0m25s Loss: 0.6895 Dice: 0.4438\n",
      "[5/10][8/20] Elapsed_time: 0m28s Loss: 0.6893 Dice: 0.4230\n",
      "[5/10][9/20] Elapsed_time: 0m31s Loss: 0.6912 Dice: 0.3879\n",
      "[5/10][10/20] Elapsed_time: 0m34s Loss: 0.6894 Dice: 0.4437\n",
      "[5/10][11/20] Elapsed_time: 0m39s Loss: 0.6900 Dice: 0.3940\n",
      "[5/10][12/20] Elapsed_time: 0m42s Loss: 0.6901 Dice: 0.4383\n",
      "[5/10][13/20] Elapsed_time: 0m45s Loss: 0.6908 Dice: 0.4245\n",
      "[5/10][14/20] Elapsed_time: 0m48s Loss: 0.6916 Dice: 0.4455\n",
      "[5/10][15/20] Elapsed_time: 0m52s Loss: 0.6935 Dice: 0.3288\n",
      "[5/10][16/20] Elapsed_time: 0m55s Loss: 0.6932 Dice: 0.3547\n",
      "[5/10][17/20] Elapsed_time: 0m59s Loss: 0.6894 Dice: 0.4294\n",
      "[5/10][18/20] Elapsed_time: 1m2s Loss: 0.6924 Dice: 0.3774\n",
      "[5/10][19/20] Elapsed_time: 1m6s Loss: 0.6906 Dice: 0.3548\n",
      "Elapsed_time: 0m3s Val dice: 0.5210\n",
      "[6/10][0/20] Elapsed_time: 0m4s Loss: 0.6895 Dice: 0.4529\n",
      "[6/10][1/20] Elapsed_time: 0m7s Loss: 0.6915 Dice: 0.3686\n",
      "[6/10][2/20] Elapsed_time: 0m11s Loss: 0.6900 Dice: 0.3038\n",
      "[6/10][3/20] Elapsed_time: 0m15s Loss: 0.6900 Dice: 0.4549\n",
      "[6/10][4/20] Elapsed_time: 0m18s Loss: 0.6915 Dice: 0.3560\n",
      "[6/10][5/20] Elapsed_time: 0m22s Loss: 0.6897 Dice: 0.4284\n",
      "[6/10][6/20] Elapsed_time: 0m25s Loss: 0.6911 Dice: 0.4147\n",
      "[6/10][7/20] Elapsed_time: 0m28s Loss: 0.6897 Dice: 0.4730\n",
      "[6/10][8/20] Elapsed_time: 0m32s Loss: 0.6899 Dice: 0.4780\n",
      "[6/10][9/20] Elapsed_time: 0m37s Loss: 0.6904 Dice: 0.4126\n",
      "[6/10][10/20] Elapsed_time: 0m40s Loss: 0.6913 Dice: 0.4236\n",
      "[6/10][11/20] Elapsed_time: 0m44s Loss: 0.6919 Dice: 0.4890\n",
      "[6/10][12/20] Elapsed_time: 0m48s Loss: 0.6886 Dice: 0.4773\n",
      "[6/10][13/20] Elapsed_time: 0m51s Loss: 0.6901 Dice: 0.3346\n",
      "[6/10][14/20] Elapsed_time: 0m55s Loss: 0.6903 Dice: 0.4538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10][15/20] Elapsed_time: 0m58s Loss: 0.6919 Dice: 0.3433\n",
      "[6/10][16/20] Elapsed_time: 1m1s Loss: 0.6922 Dice: 0.3364\n",
      "[6/10][17/20] Elapsed_time: 1m4s Loss: 0.6926 Dice: 0.3951\n",
      "[6/10][18/20] Elapsed_time: 1m7s Loss: 0.6913 Dice: 0.3379\n",
      "[6/10][19/20] Elapsed_time: 1m11s Loss: 0.6907 Dice: 0.4815\n",
      "Elapsed_time: 0m3s Val dice: 0.5215\n",
      "[7/10][0/20] Elapsed_time: 0m2s Loss: 0.6921 Dice: 0.4124\n",
      "[7/10][1/20] Elapsed_time: 0m7s Loss: 0.6908 Dice: 0.3360\n",
      "[7/10][2/20] Elapsed_time: 0m11s Loss: 0.6896 Dice: 0.4760\n",
      "[7/10][3/20] Elapsed_time: 0m16s Loss: 0.6879 Dice: 0.4241\n",
      "[7/10][4/20] Elapsed_time: 0m19s Loss: 0.6905 Dice: 0.3461\n",
      "[7/10][5/20] Elapsed_time: 0m22s Loss: 0.6903 Dice: 0.3344\n",
      "[7/10][6/20] Elapsed_time: 0m24s Loss: 0.6912 Dice: 0.4676\n",
      "[7/10][7/20] Elapsed_time: 0m28s Loss: 0.6864 Dice: 0.4859\n",
      "[7/10][8/20] Elapsed_time: 0m31s Loss: 0.6910 Dice: 0.4660\n",
      "[7/10][9/20] Elapsed_time: 0m35s Loss: 0.6922 Dice: 0.4076\n",
      "[7/10][10/20] Elapsed_time: 0m38s Loss: 0.6930 Dice: 0.3527\n",
      "[7/10][11/20] Elapsed_time: 0m41s Loss: 0.6907 Dice: 0.4865\n",
      "[7/10][12/20] Elapsed_time: 0m44s Loss: 0.6895 Dice: 0.4870\n",
      "[7/10][13/20] Elapsed_time: 0m47s Loss: 0.6914 Dice: 0.3796\n",
      "[7/10][14/20] Elapsed_time: 0m51s Loss: 0.6881 Dice: 0.4575\n",
      "[7/10][15/20] Elapsed_time: 0m55s Loss: 0.6921 Dice: 0.3065\n",
      "[7/10][16/20] Elapsed_time: 0m57s Loss: 0.6928 Dice: 0.3521\n",
      "[7/10][17/20] Elapsed_time: 1m2s Loss: 0.6924 Dice: 0.3543\n",
      "[7/10][18/20] Elapsed_time: 1m6s Loss: 0.6915 Dice: 0.3632\n",
      "[7/10][19/20] Elapsed_time: 1m11s Loss: 0.6932 Dice: 0.4147\n",
      "Elapsed_time: 0m3s Val dice: 0.5431\n",
      "[8/10][0/20] Elapsed_time: 0m4s Loss: 0.6914 Dice: 0.3552\n",
      "[8/10][1/20] Elapsed_time: 0m8s Loss: 0.6912 Dice: 0.4293\n",
      "[8/10][2/20] Elapsed_time: 0m12s Loss: 0.6906 Dice: 0.4260\n",
      "[8/10][3/20] Elapsed_time: 0m16s Loss: 0.6902 Dice: 0.3897\n",
      "[8/10][4/20] Elapsed_time: 0m21s Loss: 0.6914 Dice: 0.4369\n",
      "[8/10][5/20] Elapsed_time: 0m25s Loss: 0.6906 Dice: 0.3728\n",
      "[8/10][6/20] Elapsed_time: 0m29s Loss: 0.6915 Dice: 0.4952\n",
      "[8/10][7/20] Elapsed_time: 0m32s Loss: 0.6914 Dice: 0.3326\n",
      "[8/10][8/20] Elapsed_time: 0m36s Loss: 0.6916 Dice: 0.2163\n",
      "[8/10][9/20] Elapsed_time: 0m41s Loss: 0.6897 Dice: 0.3293\n",
      "[8/10][10/20] Elapsed_time: 0m43s Loss: 0.6910 Dice: 0.4185\n",
      "[8/10][11/20] Elapsed_time: 0m47s Loss: 0.6895 Dice: 0.3864\n",
      "[8/10][12/20] Elapsed_time: 0m51s Loss: 0.6901 Dice: 0.4216\n",
      "[8/10][13/20] Elapsed_time: 0m55s Loss: 0.6917 Dice: 0.3851\n",
      "[8/10][14/20] Elapsed_time: 0m58s Loss: 0.6894 Dice: 0.4262\n",
      "[8/10][15/20] Elapsed_time: 1m3s Loss: 0.6901 Dice: 0.4120\n",
      "[8/10][16/20] Elapsed_time: 1m6s Loss: 0.6900 Dice: 0.4215\n",
      "[8/10][17/20] Elapsed_time: 1m10s Loss: 0.6913 Dice: 0.4094\n",
      "[8/10][18/20] Elapsed_time: 1m13s Loss: 0.6912 Dice: 0.4075\n",
      "[8/10][19/20] Elapsed_time: 1m17s Loss: 0.6916 Dice: 0.3590\n",
      "Elapsed_time: 0m3s Val dice: 0.5556\n",
      "[9/10][0/20] Elapsed_time: 0m3s Loss: 0.6904 Dice: 0.3705\n",
      "[9/10][1/20] Elapsed_time: 0m7s Loss: 0.6877 Dice: 0.5311\n",
      "[9/10][2/20] Elapsed_time: 0m10s Loss: 0.6889 Dice: 0.4431\n",
      "[9/10][3/20] Elapsed_time: 0m14s Loss: 0.6900 Dice: 0.4698\n",
      "[9/10][4/20] Elapsed_time: 0m18s Loss: 0.6898 Dice: 0.4684\n",
      "[9/10][5/20] Elapsed_time: 0m21s Loss: 0.6917 Dice: 0.4112\n",
      "[9/10][6/20] Elapsed_time: 0m24s Loss: 0.6917 Dice: 0.4071\n",
      "[9/10][7/20] Elapsed_time: 0m28s Loss: 0.6909 Dice: 0.4113\n",
      "[9/10][8/20] Elapsed_time: 0m32s Loss: 0.6926 Dice: 0.3539\n",
      "[9/10][9/20] Elapsed_time: 0m36s Loss: 0.6917 Dice: 0.4129\n",
      "[9/10][10/20] Elapsed_time: 0m41s Loss: 0.6895 Dice: 0.4355\n",
      "[9/10][11/20] Elapsed_time: 0m44s Loss: 0.6900 Dice: 0.3747\n",
      "[9/10][12/20] Elapsed_time: 0m47s Loss: 0.6900 Dice: 0.4595\n",
      "[9/10][13/20] Elapsed_time: 0m50s Loss: 0.6910 Dice: 0.4526\n",
      "[9/10][14/20] Elapsed_time: 0m53s Loss: 0.6895 Dice: 0.4498\n",
      "[9/10][15/20] Elapsed_time: 0m56s Loss: 0.6898 Dice: 0.3774\n",
      "[9/10][16/20] Elapsed_time: 0m60s Loss: 0.6906 Dice: 0.3748\n",
      "[9/10][17/20] Elapsed_time: 1m2s Loss: 0.6921 Dice: 0.3462\n",
      "[9/10][18/20] Elapsed_time: 1m5s Loss: 0.6910 Dice: 0.3709\n",
      "[9/10][19/20] Elapsed_time: 1m8s Loss: 0.6897 Dice: 0.4128\n",
      "Elapsed_time: 0m3s Val dice: 0.5574\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "save_every=10\n",
    "all_error = np.zeros(0)\n",
    "all_error_L1 = np.zeros(0)\n",
    "all_error_dice = np.zeros(0)\n",
    "all_dice = np.zeros(0)\n",
    "all_val_dice = np.zeros(1)\n",
    "all_val_error = np.zeros(0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ##########\n",
    "    # Train\n",
    "    ##########\n",
    "    t0 = time.time()\n",
    "    for i, (data, label) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        label= label.to(device)\n",
    "        # setting your network to train will ensure that parameters will be updated during training, \n",
    "        # and that dropout will be used\n",
    "        net.train()\n",
    "        net.zero_grad()\n",
    "\n",
    "        target_real = torch.ones(data.size()[0])\n",
    "        batch_size = data.size()[0]\n",
    "        pred = net(data)\n",
    "        \n",
    "        # dice loss = 1-dice_coeff\n",
    "        # ----------------------------------------------- task 3 ------------------------------------------------------------\n",
    "        # Task 3: change loss function here\n",
    "        err = 1- dice_coeff(pred, label)\n",
    "        err = loss_BCE(pred, label)\n",
    "        # -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        dice_value = dice_coeff(pred, label).item()\n",
    "\n",
    "        err.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        time_elapsed = time.time() - t0\n",
    "        print('[{:d}/{:d}][{:d}/{:d}] Elapsed_time: {:.0f}m{:.0f}s Loss: {:.4f} Dice: {:.4f}'\n",
    "              .format(epoch, epochs, i, len(train_loader), time_elapsed // 60, time_elapsed % 60,\n",
    "                      err.item(), dice_value))\n",
    "\n",
    "        if i % save_every == 0:\n",
    "            # setting your network to eval mode to remove dropout during testing\n",
    "            net.eval()\n",
    "\n",
    "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_train_data.png' % (path, epoch, i),\n",
    "                                  normalize=True)\n",
    "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_train_label.png' % (path, epoch, i),\n",
    "                                  normalize=True)\n",
    "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_train_pred.png' % (path, epoch, i),\n",
    "                                  normalize=True)\n",
    "\n",
    "            error = err.item()\n",
    "\n",
    "            all_error = np.append(all_error, error)\n",
    "            all_dice = np.append(all_dice, dice_value)\n",
    "\n",
    "    # #############\n",
    "    # # Validation\n",
    "    # #############\n",
    "    mean_error = np.zeros(0)\n",
    "    mean_dice = np.zeros(0)\n",
    "    t0 = time.time()\n",
    "    for i, (data, label) in enumerate(val_loader):\n",
    "        data = data.to(device)\n",
    "        label= label.to(device)\n",
    "        net.eval()\n",
    "        batch_size = data.size()[0]\n",
    "\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        pred = net(data)\n",
    "        \n",
    "        # ----------------------------------------------- task 3 ------------------------------------------------------------\n",
    "        # Task 3: change loss function here\n",
    "        err = 1-dice_coeff(pred, label)\n",
    "        # err = loss_BCE(pred, label)\n",
    "        # -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # compare generated image to data-  metric\n",
    "        dice_value = dice_coeff(pred, label).item()\n",
    "\n",
    "        if i == 0:\n",
    "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_val_data.png' % (path, epoch, i),\n",
    "                              normalize=True)\n",
    "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_val_label.png' % (path, epoch, i),\n",
    "                              normalize=True)\n",
    "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_val_pred.png' % (path, epoch, i),\n",
    "                              normalize=True)\n",
    "\n",
    "        error = err.item()\n",
    "        mean_error = np.append(mean_error, error)\n",
    "        mean_dice = np.append(mean_dice, dice_value)\n",
    "\n",
    "    all_val_error = np.append(all_val_error, np.mean(mean_error))\n",
    "    all_val_dice = np.append(all_val_dice, np.mean(mean_dice))\n",
    "\n",
    "    time_elapsed = time.time() - t0\n",
    "\n",
    "    print('Elapsed_time: {:.0f}m{:.0f}s Val dice: {:.4f}'\n",
    "          .format(time_elapsed // 60, time_elapsed % 60, mean_dice.mean()))\n",
    "    \n",
    "    \n",
    "    num_it_per_epoch_train = ((train_loader.dataset.x_data.shape[0] * (1 - 0.2)) // (\n",
    "            save_every * batch_size)) + 1\n",
    "    epochs_train = np.arange(1,all_error.size+1) / num_it_per_epoch_train\n",
    "    epochs_val = np.arange(0,all_val_dice.size)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs_val, all_val_dice, label='dice_val')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend()\n",
    "    plt.title('Dice score')\n",
    "    plt.savefig(path + '/dice_val.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "the results are saved to a folder \\results - so please check that:\n",
    "\n",
    "The results are saved per epoch for both training and validation, and are saved as the \n",
    "1. real data, \n",
    "2. binary labels, \n",
    "3. predicted labels. \n",
    "\n",
    "In this example since we trained on a small sample of the data (100 crops) the results are far from optimal, and are likely to overfit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.3\n",
    "\n",
    "1. Edit the code to run on GPU (**hint** use .to(device)\n",
    "2. Change the dice loss to a cross entropy loss in the code - is dice loss or cross entropy loss better?\n",
    "\n",
    "**Note down your dice validation scores for each experiment, then change**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
