{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ResNet for classification - CIFAR10\n",
    "\n",
    "### Task 5 - run ResNet on CIFAR10 for classification \n",
    "\n",
    "Create a ResNet network and train for classification on the CIFAR10 dataset.\n",
    "\n",
    "Here are some example images from the CIFAR10 datasets- we have 10 classes:\n",
    "\n",
    "![cifar10](cifar10.jpg)\n",
    "source: https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\n",
    "\n",
    "You can load the CIFAR10 dataset using torchvision in the following way:\n",
    "```python\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=8,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "```\n",
    "You can use this tutorial as a reference for training on CIFAR10 - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "Remember to define your loss function, optimizer, dataloaders, and your resnet network. \n",
    "Then run the training and testing, same as with MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------task 5 -----------------------------------------------------\n",
    "# Task 5: Train and test ResNet on CIFAR10 dataset for classification\n",
    "# hints: define your resnet network, loss function, optimizer and dataloaders. \n",
    "# Then you can run the same training and testing code as above.\n",
    "# ----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 169877504/170498071 [00:32<00:00, 6205325.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=8,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNet18' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f85c1ff27d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresnet_cifar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresnet_cifar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_cifar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResNet18' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "resnet_cifar = ResNet18(in_channels=3)\n",
    "resnet_cifar = resnet_cifar.to(device)\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "loss_fun = loss_fun.to(device)\n",
    "\n",
    "optimizer = optim.SGD(resnet_cifar.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "170500096it [00:50, 6205325.08it/s]                               "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs): \n",
    "\n",
    "    # enumerate can be used to output iteration index i, as well as the data \n",
    "    for i, (data, labels) in enumerate(train_loader, 0):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #feed the input and acquire the output from network\n",
    "        outputs = resnet_cifar(data)\n",
    "\n",
    "        #calculating the predicted and the expected loss\n",
    "        loss = loss_fun(outputs, labels)\n",
    "\n",
    "        #compute the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        #update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        ce_loss = loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                 (epoch + 1, i + 1, ce_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an iterator from test_loader\n",
    "#Get a batch of testing images\n",
    "test_iterator = iter(test_loader)\n",
    "images, labels = test_iterator.next()\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "y_score = resnet_cifar(images)\n",
    "# get predicted class from the class probabilities\n",
    "_, y_pred = torch.max(y_score, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[y_pred[j]] for j in range(8)))\n",
    "\n",
    "# plot y_score - true label (t) vs predicted label (p)\n",
    "fig2 = plt.figure()\n",
    "for i in range(8):\n",
    "    fig2.add_subplot(rows, columns, i+1)\n",
    "    plt.title('t: ' + classes[labels[i].cpu()] + ' p: ' + classes[y_pred[i].cpu()])\n",
    "    img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
    "    img = torchvision.transforms.ToPILImage()(img.cpu())\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = labels.data.cpu().numpy()\n",
    "y_pred = y_pred.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "print('accuracy:', accuracy, ', f1 score:', f1, ', precision:', precision, ', recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image segmentation with pytorch using U-net\n",
    "\n",
    "U-net was first developed in 2015 by Ronneberger et al., as a segmentation network for biomedical image analysis.\n",
    "It has been extremely successful, with 9,000+ citations, and many new methods that have used the U-net architecture since.\n",
    "\n",
    "\n",
    "The architecture of U-net is based on the idea of using skip connections (i.e. concatenating) at different levels of the network to retain high, and low level features.\n",
    "\n",
    "Here is the architecture of a U-net:\n",
    "\n",
    "---\n",
    "\n",
    "![U-net](unet.png)\n",
    "Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-photon microscopy dataset of cortical axons\n",
    "\n",
    "In this tutorial we use a dataset of cortical neurons with their corresponding segmentation binary labels.\n",
    "\n",
    "These images were collected using in-vivo two-photon microscopy from the mouse somatosensory cortex. To generate the 2D images, a max projection was used over the 3D stack. The labels are binary segmentation maps of the axons.\n",
    "\n",
    "Here we will use 100 [64x64] crops during training and validation. \n",
    "\n",
    "These are some example images [256x256] from the original dataset:\n",
    "![axon_dataset](axon_dataset.png)\n",
    "\n",
    "Bass, Cher, et al. \"Image synthesis with a convolutional capsule generative adversarial network.\" Medical Imaging with Deep Learning (2019).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load modules\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from AxonDataset import AxonDataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "timestr = time.strftime(\"%d%m%Y-%H%M\")\n",
    "__location__ = os.path.realpath(\n",
    "    os.path.join(os.getcwd(), os.path.dirname('__file__')))\n",
    "\n",
    "path = os.path.join(__location__,'results')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "# Define your batch_size\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataloader\n",
    "\n",
    "In this example, a custom dataloader was created, and we import it from `AxonDataset.py`\n",
    "\n",
    "we create a dataset, and split into a train and validation set with 80%, 20% split\n",
    "\n",
    "### Task 1\n",
    "\n",
    "create a list of random indices for the train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we create a dataloader for our example dataset- two photon microscopy with axons\n",
    "axon_dataset = AxonDataset(data_name='org64', type='train')\n",
    "\n",
    "# -----------------------------------------------------task 1----------------------------------------------------------------\n",
    "# Task 1: create a random list of incides for training and testing with a 80%,20% split\n",
    "\n",
    "# We need to further split our training dataset into training and validation sets.\n",
    "# Define the indices\n",
    "indices = list(range(len(axon_dataset)))  # start with all the indices in training set\n",
    "split = int(len(indices)*0.2)  # define the split size\n",
    "\n",
    "# Get indices for train and validation datasets, and split the data\n",
    "validation_idx = np.random.choice(indices, size=split, replace=False)\n",
    "train_idx = list(set(indices) - set(validation_idx))\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# feed indices into the sampler\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "\n",
    "# Create a dataloader instance \n",
    "train_loader = torch.utils.data.DataLoader(axon_dataset, batch_size = batch_size,\n",
    "                                           sampler=train_sampler) \n",
    "val_loader = torch.utils.data.DataLoader(axon_dataset, batch_size = batch_size,\n",
    "                                        sampler=validation_sampler) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a U-net \n",
    "\n",
    "We next build our u-net network.\n",
    "\n",
    "First we define a layer `double_conv` that performs 2 sets of convolution followed by ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define U-net\n",
    "def double_conv(in_channels, out_channels, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=padding),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define neural network\n",
    "We then define our U-net network.\n",
    "\n",
    "We initialise all the different layers in the network in `__init__`:\n",
    "1. `self.dconv_down1` is a double convolutional layer\n",
    "2. `self.maxpool` is a max pooling layer that is used to reduce the size of the input, and decrease the reptive field\n",
    "3. `self.upsample` is an upsampling layer that is used to increase the size of the input\n",
    "4. `dropout` is a dropout layer that is applied to regulise the training\n",
    "5. `dconv_up4` is also a double convolutional layer- note that it takes in additional channels from previous layers (i.e. the skip connections).\n",
    "\n",
    "skip connection are easily implemented by concatenating the result of a previous convolution with the current input, \n",
    "\n",
    "using e.g. `torch.cat([x, conv4], dim=1)`\n",
    "\n",
    "### Task 2 - implement skip connections\n",
    "Implement skip connections for conv3, conv2, and conv1.\n",
    "\n",
    "See conv4 example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dconv_down1 = double_conv(1, 32)\n",
    "        self.dconv_down2 = double_conv(32, 64)\n",
    "        self.dconv_down3 = double_conv(64, 128)\n",
    "        self.dconv_down4 = double_conv(128, 256)\n",
    "        self.dconv_down5 = double_conv(256, 512)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.dconv_up4 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up3 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up2 = double_conv(128 + 64, 64)\n",
    "        self.dconv_up1 = double_conv(64 + 32, 32)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(32, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        conv1 = self.dropout(conv1)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        conv2 = self.dropout(conv2)\n",
    "        x = self.maxpool(conv2)\n",
    "\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        conv3 = self.dropout(conv3)\n",
    "        x = self.maxpool(conv3)\n",
    "\n",
    "        conv4 = self.dconv_down4(x)\n",
    "        conv4 = self.dropout(conv4)\n",
    "        x = self.maxpool(conv4)\n",
    "\n",
    "        conv5 = self.dconv_down5(x)\n",
    "        conv5 = self.dropout(conv5)\n",
    "\n",
    "        x = self.upsample(conv5)\n",
    "        \n",
    "        # example of skip connection with conv4\n",
    "        x = torch.cat([x, conv4], dim=1)\n",
    "        \n",
    "        x = self.dconv_up4(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        # --------------------------------------------------- task 2 ----------------------------------------------------------\n",
    "        # Task 2: implement skip connection with conv3\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        # --------------------------------------------------- task 2 ----------------------------------------------------------\n",
    "        # Task 2: implement skip connection with conv2\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        # --------------------------------------------------- task 2 ----------------------------------------------------------\n",
    "        # Task 2: implement skip connection with conv1\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        out = F.sigmoid(self.conv_last(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we initialise the network with a previously trained network by loading the weights\n",
    "\n",
    "*for practical reasons training this network from scratch will take too long, and require large computational resources*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise network - and load weights\n",
    "net = UNet()\n",
    "net.load_state_dict(torch.load(path+'/'+'model.pt')) #this function loads a pretrained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an appropriate loss function\n",
    "We next define our loss function - in this case we use Dice loss, a commonly used loss for image segmentation.\n",
    "\n",
    "The Dice coefficient can be used as a loss function, and is essentially a measure of overlap between two samples.\n",
    "\n",
    "Dice is in the range of 0 to 1, where a Dice coefficient of 1 denotes perfect and complete overlap. The Dice coefficient was originally developed for binary data, and can be calculated as:\n",
    "\n",
    "$Dice = \\dfrac{2|A\\cap B|}{|A| + |B|}$\n",
    "\n",
    "where $|A\\cap B|$ represents the common elements between sets $A$ and $B$, and $|A|$ represents the number of elements in set $A$ (and likewise for set $B$).\n",
    "\n",
    "For the case of evaluating a Dice coefficient on predicted segmentation masks, we can approximate  $|A\\cap B|$ as the element-wise multiplication between the prediction and target mask, and then sum the resulting matrix.\n",
    "\n",
    "An **alternative loss** function would be pixel-wise cross entropy loss. It would examine each pixel individually, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice loss\n",
    "def dice_coeff(pred, target):\n",
    "    \"\"\"This definition generalize to real valued pred and target vector.\n",
    "    This should be differentiable.\n",
    "    pred: tensor with first dimension as batch\n",
    "    target: tensor with first dimension as batch\n",
    "    \"\"\"\n",
    "\n",
    "    smooth = 1.\n",
    "    epsilon = 10e-8\n",
    "\n",
    "    # have to use contiguous since they may from a torch.view op\n",
    "    iflat = pred.contiguous().view(-1)\n",
    "    tflat = target.contiguous().view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    A_sum = torch.sum(iflat * iflat)\n",
    "    B_sum = torch.sum(tflat * tflat)\n",
    "\n",
    "    dice = (2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "    dice = dice.mean(dim=0)\n",
    "    dice = torch.clamp(dice, 0, 1.0-epsilon)\n",
    "\n",
    "    return  dice\n",
    "\n",
    "# cross entropy loss\n",
    "loss_BCE = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as before, we define the optimiser to train our network - here we use Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define your optimiser\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=1e-05, betas=(0.5, 0.999))\n",
    "optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating our segmentation network\n",
    "We next train and evaluate our network \n",
    "\n",
    "note that the results are saved to a folder \\results - so please check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cb19/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/home/cb19/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][0/20] Elapsed_time: 0m1s Loss: 0.6918 Dice: 0.3308\n",
      "[0/10][1/20] Elapsed_time: 0m1s Loss: 0.6926 Dice: 0.3762\n",
      "[0/10][2/20] Elapsed_time: 0m2s Loss: 0.6926 Dice: 0.4082\n",
      "[0/10][3/20] Elapsed_time: 0m3s Loss: 0.6921 Dice: 0.3713\n",
      "[0/10][4/20] Elapsed_time: 0m3s Loss: 0.6929 Dice: 0.4716\n",
      "[0/10][5/20] Elapsed_time: 0m4s Loss: 0.6918 Dice: 0.3139\n",
      "[0/10][6/20] Elapsed_time: 0m5s Loss: 0.6902 Dice: 0.3906\n",
      "[0/10][7/20] Elapsed_time: 0m5s Loss: 0.6906 Dice: 0.2503\n",
      "[0/10][8/20] Elapsed_time: 0m6s Loss: 0.6891 Dice: 0.4556\n",
      "[0/10][9/20] Elapsed_time: 0m7s Loss: 0.6932 Dice: 0.4262\n",
      "[0/10][10/20] Elapsed_time: 0m7s Loss: 0.6914 Dice: 0.3525\n",
      "[0/10][11/20] Elapsed_time: 0m8s Loss: 0.6917 Dice: 0.3306\n",
      "[0/10][12/20] Elapsed_time: 0m9s Loss: 0.6912 Dice: 0.3525\n",
      "[0/10][13/20] Elapsed_time: 0m9s Loss: 0.6923 Dice: 0.3962\n",
      "[0/10][14/20] Elapsed_time: 0m10s Loss: 0.6909 Dice: 0.4460\n",
      "[0/10][15/20] Elapsed_time: 0m11s Loss: 0.6908 Dice: 0.4187\n",
      "[0/10][16/20] Elapsed_time: 0m11s Loss: 0.6912 Dice: 0.4453\n",
      "[0/10][17/20] Elapsed_time: 0m12s Loss: 0.6889 Dice: 0.4754\n",
      "[0/10][18/20] Elapsed_time: 0m13s Loss: 0.6910 Dice: 0.4276\n",
      "[0/10][19/20] Elapsed_time: 0m13s Loss: 0.6919 Dice: 0.3787\n",
      "Elapsed_time: 0m1s Val dice: 0.5270\n",
      "[1/10][0/20] Elapsed_time: 0m1s Loss: 0.6915 Dice: 0.3674\n",
      "[1/10][1/20] Elapsed_time: 0m2s Loss: 0.6910 Dice: 0.3627\n",
      "[1/10][2/20] Elapsed_time: 0m2s Loss: 0.6908 Dice: 0.4468\n",
      "[1/10][3/20] Elapsed_time: 0m3s Loss: 0.6933 Dice: 0.3507\n",
      "[1/10][4/20] Elapsed_time: 0m4s Loss: 0.6918 Dice: 0.4068\n",
      "[1/10][5/20] Elapsed_time: 0m4s Loss: 0.6899 Dice: 0.4487\n",
      "[1/10][6/20] Elapsed_time: 0m5s Loss: 0.6915 Dice: 0.2774\n",
      "[1/10][7/20] Elapsed_time: 0m6s Loss: 0.6878 Dice: 0.5183\n",
      "[1/10][8/20] Elapsed_time: 0m6s Loss: 0.6908 Dice: 0.4952\n",
      "[1/10][9/20] Elapsed_time: 0m7s Loss: 0.6904 Dice: 0.5510\n",
      "[1/10][10/20] Elapsed_time: 0m8s Loss: 0.6920 Dice: 0.3382\n",
      "[1/10][11/20] Elapsed_time: 0m8s Loss: 0.6913 Dice: 0.3792\n",
      "[1/10][12/20] Elapsed_time: 0m9s Loss: 0.6932 Dice: 0.3421\n",
      "[1/10][13/20] Elapsed_time: 0m9s Loss: 0.6905 Dice: 0.4519\n",
      "[1/10][14/20] Elapsed_time: 0m10s Loss: 0.6927 Dice: 0.4929\n",
      "[1/10][15/20] Elapsed_time: 0m11s Loss: 0.6903 Dice: 0.4370\n",
      "[1/10][16/20] Elapsed_time: 0m12s Loss: 0.6899 Dice: 0.4743\n",
      "[1/10][17/20] Elapsed_time: 0m12s Loss: 0.6918 Dice: 0.3555\n",
      "[1/10][18/20] Elapsed_time: 0m13s Loss: 0.6906 Dice: 0.3709\n",
      "[1/10][19/20] Elapsed_time: 0m14s Loss: 0.6904 Dice: 0.3124\n",
      "Elapsed_time: 0m1s Val dice: 0.5015\n",
      "[2/10][0/20] Elapsed_time: 0m1s Loss: 0.6889 Dice: 0.4133\n",
      "[2/10][1/20] Elapsed_time: 0m2s Loss: 0.6906 Dice: 0.4001\n",
      "[2/10][2/20] Elapsed_time: 0m2s Loss: 0.6892 Dice: 0.4105\n",
      "[2/10][3/20] Elapsed_time: 0m3s Loss: 0.6917 Dice: 0.4483\n",
      "[2/10][4/20] Elapsed_time: 0m3s Loss: 0.6899 Dice: 0.4398\n",
      "[2/10][5/20] Elapsed_time: 0m4s Loss: 0.6887 Dice: 0.4494\n",
      "[2/10][6/20] Elapsed_time: 0m4s Loss: 0.6905 Dice: 0.4305\n",
      "[2/10][7/20] Elapsed_time: 0m5s Loss: 0.6946 Dice: 0.4304\n",
      "[2/10][8/20] Elapsed_time: 0m6s Loss: 0.6918 Dice: 0.4621\n",
      "[2/10][9/20] Elapsed_time: 0m6s Loss: 0.6902 Dice: 0.4726\n",
      "[2/10][10/20] Elapsed_time: 0m7s Loss: 0.6901 Dice: 0.4765\n",
      "[2/10][11/20] Elapsed_time: 0m8s Loss: 0.6928 Dice: 0.2950\n",
      "[2/10][12/20] Elapsed_time: 0m8s Loss: 0.6904 Dice: 0.3685\n",
      "[2/10][13/20] Elapsed_time: 0m9s Loss: 0.6912 Dice: 0.3994\n",
      "[2/10][14/20] Elapsed_time: 0m10s Loss: 0.6902 Dice: 0.3605\n",
      "[2/10][15/20] Elapsed_time: 0m11s Loss: 0.6911 Dice: 0.4091\n",
      "[2/10][16/20] Elapsed_time: 0m11s Loss: 0.6915 Dice: 0.3539\n",
      "[2/10][17/20] Elapsed_time: 0m12s Loss: 0.6922 Dice: 0.3880\n",
      "[2/10][18/20] Elapsed_time: 0m13s Loss: 0.6905 Dice: 0.3748\n",
      "[2/10][19/20] Elapsed_time: 0m13s Loss: 0.6925 Dice: 0.2506\n",
      "Elapsed_time: 0m1s Val dice: 0.5031\n",
      "[3/10][0/20] Elapsed_time: 0m1s Loss: 0.6913 Dice: 0.4394\n",
      "[3/10][1/20] Elapsed_time: 0m1s Loss: 0.6938 Dice: 0.2915\n",
      "[3/10][2/20] Elapsed_time: 0m2s Loss: 0.6899 Dice: 0.3742\n",
      "[3/10][3/20] Elapsed_time: 0m3s Loss: 0.6896 Dice: 0.4032\n",
      "[3/10][4/20] Elapsed_time: 0m4s Loss: 0.6916 Dice: 0.2682\n",
      "[3/10][5/20] Elapsed_time: 0m4s Loss: 0.6886 Dice: 0.4266\n",
      "[3/10][6/20] Elapsed_time: 0m5s Loss: 0.6905 Dice: 0.4237\n",
      "[3/10][7/20] Elapsed_time: 0m6s Loss: 0.6914 Dice: 0.4615\n",
      "[3/10][8/20] Elapsed_time: 0m7s Loss: 0.6920 Dice: 0.3913\n",
      "[3/10][9/20] Elapsed_time: 0m7s Loss: 0.6912 Dice: 0.4169\n",
      "[3/10][10/20] Elapsed_time: 0m8s Loss: 0.6906 Dice: 0.3618\n",
      "[3/10][11/20] Elapsed_time: 0m9s Loss: 0.6899 Dice: 0.4315\n",
      "[3/10][12/20] Elapsed_time: 0m10s Loss: 0.6911 Dice: 0.4677\n",
      "[3/10][13/20] Elapsed_time: 0m10s Loss: 0.6904 Dice: 0.4273\n",
      "[3/10][14/20] Elapsed_time: 0m11s Loss: 0.6906 Dice: 0.4687\n",
      "[3/10][15/20] Elapsed_time: 0m12s Loss: 0.6915 Dice: 0.4441\n",
      "[3/10][16/20] Elapsed_time: 0m12s Loss: 0.6912 Dice: 0.4025\n",
      "[3/10][17/20] Elapsed_time: 0m13s Loss: 0.6913 Dice: 0.3339\n",
      "[3/10][18/20] Elapsed_time: 0m14s Loss: 0.6916 Dice: 0.4763\n",
      "[3/10][19/20] Elapsed_time: 0m14s Loss: 0.6892 Dice: 0.4683\n",
      "Elapsed_time: 0m1s Val dice: 0.5148\n",
      "[4/10][0/20] Elapsed_time: 0m1s Loss: 0.6912 Dice: 0.3313\n",
      "[4/10][1/20] Elapsed_time: 0m1s Loss: 0.6911 Dice: 0.3723\n",
      "[4/10][2/20] Elapsed_time: 0m2s Loss: 0.6901 Dice: 0.4041\n",
      "[4/10][3/20] Elapsed_time: 0m3s Loss: 0.6909 Dice: 0.3613\n",
      "[4/10][4/20] Elapsed_time: 0m4s Loss: 0.6898 Dice: 0.5083\n",
      "[4/10][5/20] Elapsed_time: 0m4s Loss: 0.6945 Dice: 0.3956\n",
      "[4/10][6/20] Elapsed_time: 0m5s Loss: 0.6903 Dice: 0.3926\n",
      "[4/10][7/20] Elapsed_time: 0m6s Loss: 0.6897 Dice: 0.4316\n",
      "[4/10][8/20] Elapsed_time: 0m7s Loss: 0.6925 Dice: 0.4999\n",
      "[4/10][9/20] Elapsed_time: 0m8s Loss: 0.6904 Dice: 0.3407\n",
      "[4/10][10/20] Elapsed_time: 0m8s Loss: 0.6904 Dice: 0.3524\n",
      "[4/10][11/20] Elapsed_time: 0m9s Loss: 0.6909 Dice: 0.2907\n",
      "[4/10][12/20] Elapsed_time: 0m10s Loss: 0.6920 Dice: 0.4129\n",
      "[4/10][13/20] Elapsed_time: 0m10s Loss: 0.6906 Dice: 0.3254\n",
      "[4/10][14/20] Elapsed_time: 0m11s Loss: 0.6903 Dice: 0.4308\n",
      "[4/10][15/20] Elapsed_time: 0m12s Loss: 0.6915 Dice: 0.3160\n",
      "[4/10][16/20] Elapsed_time: 0m13s Loss: 0.6885 Dice: 0.5212\n",
      "[4/10][17/20] Elapsed_time: 0m13s Loss: 0.6915 Dice: 0.4029\n",
      "[4/10][18/20] Elapsed_time: 0m14s Loss: 0.6913 Dice: 0.3828\n",
      "[4/10][19/20] Elapsed_time: 0m15s Loss: 0.6906 Dice: 0.3023\n",
      "Elapsed_time: 0m1s Val dice: 0.5109\n",
      "[5/10][0/20] Elapsed_time: 0m1s Loss: 0.6914 Dice: 0.4064\n",
      "[5/10][1/20] Elapsed_time: 0m1s Loss: 0.6903 Dice: 0.3718\n",
      "[5/10][2/20] Elapsed_time: 0m2s Loss: 0.6881 Dice: 0.5112\n",
      "[5/10][3/20] Elapsed_time: 0m3s Loss: 0.6892 Dice: 0.4021\n",
      "[5/10][4/20] Elapsed_time: 0m4s Loss: 0.6905 Dice: 0.4417\n",
      "[5/10][5/20] Elapsed_time: 0m5s Loss: 0.6900 Dice: 0.3830\n",
      "[5/10][6/20] Elapsed_time: 0m5s Loss: 0.6906 Dice: 0.4965\n",
      "[5/10][7/20] Elapsed_time: 0m6s Loss: 0.6912 Dice: 0.3419\n",
      "[5/10][8/20] Elapsed_time: 0m7s Loss: 0.6903 Dice: 0.3588\n",
      "[5/10][9/20] Elapsed_time: 0m8s Loss: 0.6890 Dice: 0.4436\n",
      "[5/10][10/20] Elapsed_time: 0m8s Loss: 0.6882 Dice: 0.4568\n",
      "[5/10][11/20] Elapsed_time: 0m9s Loss: 0.6905 Dice: 0.3722\n",
      "[5/10][12/20] Elapsed_time: 0m10s Loss: 0.6929 Dice: 0.4548\n",
      "[5/10][13/20] Elapsed_time: 0m11s Loss: 0.6925 Dice: 0.4157\n",
      "[5/10][14/20] Elapsed_time: 0m12s Loss: 0.6902 Dice: 0.5279\n",
      "[5/10][15/20] Elapsed_time: 0m12s Loss: 0.6907 Dice: 0.4554\n",
      "[5/10][16/20] Elapsed_time: 0m13s Loss: 0.6895 Dice: 0.5170\n",
      "[5/10][17/20] Elapsed_time: 0m14s Loss: 0.6925 Dice: 0.3891\n",
      "[5/10][18/20] Elapsed_time: 0m15s Loss: 0.6904 Dice: 0.4051\n",
      "[5/10][19/20] Elapsed_time: 0m15s Loss: 0.6901 Dice: 0.4763\n",
      "Elapsed_time: 0m1s Val dice: 0.5351\n",
      "[6/10][0/20] Elapsed_time: 0m1s Loss: 0.6889 Dice: 0.4261\n",
      "[6/10][1/20] Elapsed_time: 0m1s Loss: 0.6909 Dice: 0.4812\n",
      "[6/10][2/20] Elapsed_time: 0m2s Loss: 0.6914 Dice: 0.3512\n",
      "[6/10][3/20] Elapsed_time: 0m3s Loss: 0.6893 Dice: 0.4927\n",
      "[6/10][4/20] Elapsed_time: 0m4s Loss: 0.6905 Dice: 0.3590\n",
      "[6/10][5/20] Elapsed_time: 0m4s Loss: 0.6912 Dice: 0.3030\n",
      "[6/10][6/20] Elapsed_time: 0m5s Loss: 0.6906 Dice: 0.4450\n",
      "[6/10][7/20] Elapsed_time: 0m6s Loss: 0.6906 Dice: 0.4980\n",
      "[6/10][8/20] Elapsed_time: 0m7s Loss: 0.6923 Dice: 0.3695\n",
      "[6/10][9/20] Elapsed_time: 0m8s Loss: 0.6881 Dice: 0.5152\n",
      "[6/10][10/20] Elapsed_time: 0m9s Loss: 0.6901 Dice: 0.4142\n",
      "[6/10][11/20] Elapsed_time: 0m9s Loss: 0.6911 Dice: 0.4728\n",
      "[6/10][12/20] Elapsed_time: 0m10s Loss: 0.6900 Dice: 0.3952\n",
      "[6/10][13/20] Elapsed_time: 0m11s Loss: 0.6906 Dice: 0.3918\n",
      "[6/10][14/20] Elapsed_time: 0m11s Loss: 0.6896 Dice: 0.5547\n",
      "[6/10][15/20] Elapsed_time: 0m12s Loss: 0.6902 Dice: 0.3948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10][16/20] Elapsed_time: 0m13s Loss: 0.6902 Dice: 0.4482\n",
      "[6/10][17/20] Elapsed_time: 0m14s Loss: 0.6915 Dice: 0.3506\n",
      "[6/10][18/20] Elapsed_time: 0m14s Loss: 0.6904 Dice: 0.4503\n",
      "[6/10][19/20] Elapsed_time: 0m15s Loss: 0.6901 Dice: 0.4841\n",
      "Elapsed_time: 0m1s Val dice: 0.5425\n",
      "[7/10][0/20] Elapsed_time: 0m1s Loss: 0.6905 Dice: 0.3740\n",
      "[7/10][1/20] Elapsed_time: 0m1s Loss: 0.6903 Dice: 0.4503\n",
      "[7/10][2/20] Elapsed_time: 0m2s Loss: 0.6910 Dice: 0.4864\n",
      "[7/10][3/20] Elapsed_time: 0m3s Loss: 0.6912 Dice: 0.3169\n",
      "[7/10][4/20] Elapsed_time: 0m4s Loss: 0.6890 Dice: 0.4592\n",
      "[7/10][5/20] Elapsed_time: 0m4s Loss: 0.6898 Dice: 0.4876\n",
      "[7/10][6/20] Elapsed_time: 0m5s Loss: 0.6918 Dice: 0.3694\n",
      "[7/10][7/20] Elapsed_time: 0m6s Loss: 0.6884 Dice: 0.4061\n",
      "[7/10][8/20] Elapsed_time: 0m7s Loss: 0.6930 Dice: 0.4311\n",
      "[7/10][9/20] Elapsed_time: 0m7s Loss: 0.6907 Dice: 0.4738\n",
      "[7/10][10/20] Elapsed_time: 0m8s Loss: 0.6900 Dice: 0.5504\n",
      "[7/10][11/20] Elapsed_time: 0m9s Loss: 0.6910 Dice: 0.3993\n",
      "[7/10][12/20] Elapsed_time: 0m10s Loss: 0.6910 Dice: 0.3400\n",
      "[7/10][13/20] Elapsed_time: 0m10s Loss: 0.6900 Dice: 0.3875\n",
      "[7/10][14/20] Elapsed_time: 0m11s Loss: 0.6900 Dice: 0.4255\n",
      "[7/10][15/20] Elapsed_time: 0m12s Loss: 0.6908 Dice: 0.4119\n",
      "[7/10][16/20] Elapsed_time: 0m13s Loss: 0.6904 Dice: 0.4334\n",
      "[7/10][17/20] Elapsed_time: 0m13s Loss: 0.6891 Dice: 0.5260\n",
      "[7/10][18/20] Elapsed_time: 0m14s Loss: 0.6909 Dice: 0.4435\n",
      "[7/10][19/20] Elapsed_time: 0m15s Loss: 0.6904 Dice: 0.4519\n",
      "Elapsed_time: 0m1s Val dice: 0.5251\n",
      "[8/10][0/20] Elapsed_time: 0m1s Loss: 0.6911 Dice: 0.4358\n",
      "[8/10][1/20] Elapsed_time: 0m1s Loss: 0.6913 Dice: 0.4174\n",
      "[8/10][2/20] Elapsed_time: 0m2s Loss: 0.6903 Dice: 0.3371\n",
      "[8/10][3/20] Elapsed_time: 0m3s Loss: 0.6881 Dice: 0.4524\n",
      "[8/10][4/20] Elapsed_time: 0m4s Loss: 0.6900 Dice: 0.4284\n",
      "[8/10][5/20] Elapsed_time: 0m4s Loss: 0.6907 Dice: 0.4037\n",
      "[8/10][6/20] Elapsed_time: 0m5s Loss: 0.6899 Dice: 0.4823\n",
      "[8/10][7/20] Elapsed_time: 0m6s Loss: 0.6909 Dice: 0.3599\n",
      "[8/10][8/20] Elapsed_time: 0m6s Loss: 0.6916 Dice: 0.4184\n",
      "[8/10][9/20] Elapsed_time: 0m7s Loss: 0.6928 Dice: 0.3768\n",
      "[8/10][10/20] Elapsed_time: 0m8s Loss: 0.6911 Dice: 0.3190\n",
      "[8/10][11/20] Elapsed_time: 0m9s Loss: 0.6898 Dice: 0.4576\n",
      "[8/10][12/20] Elapsed_time: 0m10s Loss: 0.6907 Dice: 0.3654\n",
      "[8/10][13/20] Elapsed_time: 0m10s Loss: 0.6903 Dice: 0.3770\n",
      "[8/10][14/20] Elapsed_time: 0m11s Loss: 0.6889 Dice: 0.4221\n",
      "[8/10][15/20] Elapsed_time: 0m12s Loss: 0.6913 Dice: 0.5186\n",
      "[8/10][16/20] Elapsed_time: 0m13s Loss: 0.6890 Dice: 0.5171\n",
      "[8/10][17/20] Elapsed_time: 0m14s Loss: 0.6896 Dice: 0.4818\n",
      "[8/10][18/20] Elapsed_time: 0m14s Loss: 0.6896 Dice: 0.4049\n",
      "[8/10][19/20] Elapsed_time: 0m15s Loss: 0.6903 Dice: 0.4901\n",
      "Elapsed_time: 0m1s Val dice: 0.5454\n",
      "[9/10][0/20] Elapsed_time: 0m1s Loss: 0.6908 Dice: 0.4043\n",
      "[9/10][1/20] Elapsed_time: 0m2s Loss: 0.6908 Dice: 0.4395\n",
      "[9/10][2/20] Elapsed_time: 0m2s Loss: 0.6915 Dice: 0.4240\n",
      "[9/10][3/20] Elapsed_time: 0m3s Loss: 0.6907 Dice: 0.4678\n",
      "[9/10][4/20] Elapsed_time: 0m4s Loss: 0.6901 Dice: 0.4852\n",
      "[9/10][5/20] Elapsed_time: 0m4s Loss: 0.6914 Dice: 0.3806\n",
      "[9/10][6/20] Elapsed_time: 0m5s Loss: 0.6918 Dice: 0.3361\n",
      "[9/10][7/20] Elapsed_time: 0m6s Loss: 0.6905 Dice: 0.4621\n",
      "[9/10][8/20] Elapsed_time: 0m7s Loss: 0.6910 Dice: 0.3729\n",
      "[9/10][9/20] Elapsed_time: 0m8s Loss: 0.6911 Dice: 0.3800\n",
      "[9/10][10/20] Elapsed_time: 0m9s Loss: 0.6910 Dice: 0.3344\n",
      "[9/10][11/20] Elapsed_time: 0m10s Loss: 0.6897 Dice: 0.4418\n",
      "[9/10][12/20] Elapsed_time: 0m10s Loss: 0.6901 Dice: 0.4042\n",
      "[9/10][13/20] Elapsed_time: 0m11s Loss: 0.6896 Dice: 0.4338\n",
      "[9/10][14/20] Elapsed_time: 0m12s Loss: 0.6899 Dice: 0.4885\n",
      "[9/10][15/20] Elapsed_time: 0m13s Loss: 0.6899 Dice: 0.4964\n",
      "[9/10][16/20] Elapsed_time: 0m14s Loss: 0.6913 Dice: 0.4583\n",
      "[9/10][17/20] Elapsed_time: 0m14s Loss: 0.6886 Dice: 0.4603\n",
      "[9/10][18/20] Elapsed_time: 0m15s Loss: 0.6893 Dice: 0.4569\n",
      "[9/10][19/20] Elapsed_time: 0m16s Loss: 0.6881 Dice: 0.4575\n",
      "Elapsed_time: 0m1s Val dice: 0.5489\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "save_every=10\n",
    "all_error = np.zeros(0)\n",
    "all_error_L1 = np.zeros(0)\n",
    "all_error_dice = np.zeros(0)\n",
    "all_dice = np.zeros(0)\n",
    "all_val_dice = np.zeros(1)\n",
    "all_val_error = np.zeros(0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ##########\n",
    "    # Train\n",
    "    ##########\n",
    "    t0 = time.time()\n",
    "    for i, (data, label) in enumerate(train_loader):\n",
    "        \n",
    "        # setting your network to train will ensure that parameters will be updated during training, \n",
    "        # and that dropout will be used\n",
    "        net.train()\n",
    "        net.zero_grad()\n",
    "\n",
    "        target_real = torch.ones(data.size()[0])\n",
    "        batch_size = data.size()[0]\n",
    "        pred = net(data)\n",
    "        \n",
    "        # dice loss = 1-dice_coeff\n",
    "        # ----------------------------------------------- task 3 ------------------------------------------------------------\n",
    "        # Task 3: change loss function here\n",
    "        err = 1- dice_coeff(pred, label)\n",
    "        err = loss_BCE(pred, label)\n",
    "        # -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        dice_value = dice_coeff(pred, label).item()\n",
    "\n",
    "        err.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        time_elapsed = time.time() - t0\n",
    "        print('[{:d}/{:d}][{:d}/{:d}] Elapsed_time: {:.0f}m{:.0f}s Loss: {:.4f} Dice: {:.4f}'\n",
    "              .format(epoch, epochs, i, len(train_loader), time_elapsed // 60, time_elapsed % 60,\n",
    "                      err.item(), dice_value))\n",
    "\n",
    "        if i % save_every == 0:\n",
    "            # setting your network to eval mode to remove dropout during testing\n",
    "            net.eval()\n",
    "\n",
    "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_train_data.png' % (path, epoch, i),\n",
    "                                  normalize=True)\n",
    "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_train_label.png' % (path, epoch, i),\n",
    "                                  normalize=True)\n",
    "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_train_pred.png' % (path, epoch, i),\n",
    "                                  normalize=True)\n",
    "\n",
    "            error = err.item()\n",
    "\n",
    "            all_error = np.append(all_error, error)\n",
    "            all_dice = np.append(all_dice, dice_value)\n",
    "\n",
    "    # #############\n",
    "    # # Validation\n",
    "    # #############\n",
    "    mean_error = np.zeros(0)\n",
    "    mean_dice = np.zeros(0)\n",
    "    t0 = time.time()\n",
    "    for i, (data, label) in enumerate(val_loader):\n",
    "\n",
    "        net.eval()\n",
    "        batch_size = data.size()[0]\n",
    "\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        pred = net(data)\n",
    "        \n",
    "        # ----------------------------------------------- task 3 ------------------------------------------------------------\n",
    "        # Task 3: change loss function here\n",
    "        err = 1-dice_coeff(pred, label)\n",
    "        # err = loss_BCE(pred, label)\n",
    "        # -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # compare generated image to data-  metric\n",
    "        dice_value = dice_coeff(pred, label).item()\n",
    "\n",
    "        if i == 0:\n",
    "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_val_data.png' % (path, epoch, i),\n",
    "                              normalize=True)\n",
    "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_val_label.png' % (path, epoch, i),\n",
    "                              normalize=True)\n",
    "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_val_pred.png' % (path, epoch, i),\n",
    "                              normalize=True)\n",
    "\n",
    "        error = err.item()\n",
    "        mean_error = np.append(mean_error, error)\n",
    "        mean_dice = np.append(mean_dice, dice_value)\n",
    "\n",
    "    all_val_error = np.append(all_val_error, np.mean(mean_error))\n",
    "    all_val_dice = np.append(all_val_dice, np.mean(mean_dice))\n",
    "\n",
    "    time_elapsed = time.time() - t0\n",
    "\n",
    "    print('Elapsed_time: {:.0f}m{:.0f}s Val dice: {:.4f}'\n",
    "          .format(time_elapsed // 60, time_elapsed % 60, mean_dice.mean()))\n",
    "    \n",
    "    \n",
    "    num_it_per_epoch_train = ((train_loader.dataset.x_data.shape[0] * (1 - 0.2)) // (\n",
    "            save_every * batch_size)) + 1\n",
    "    epochs_train = np.arange(1,all_error.size+1) / num_it_per_epoch_train\n",
    "    epochs_val = np.arange(0,all_val_dice.size)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs_val, all_val_dice, label='dice_val')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend()\n",
    "    plt.title('Dice score')\n",
    "    plt.savefig(path + '/dice_val.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "the results are saved to a folder \\results - so please check that:\n",
    "\n",
    "The results are saved per epoch for both training and validation, and are saved as the \n",
    "1. real data, \n",
    "2. binary labels, \n",
    "3. predicted labels. \n",
    "\n",
    "In this example since we trained on a small sample of the data (100 crops) the results are far from optimal, and are likely to overfit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "1. Change the dice loss to a cross entropy loss in the code - is dice loss or cross entropy loss better?\n",
    "2. run the training with dropout - what's the effect?\n",
    "\n",
    "**Note down your dice validation scores for each experiment, then change**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
