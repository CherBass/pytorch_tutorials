{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors\n",
    "\n",
    "PyTorch is a lot like numpy. A lot of operations used to manipulate numpy arrays have their counterparts in pytorch and numpy arrays can be converted to and from pytorch *tensors*. PyTorch arrays are given the more proper mathematical name of tensors (see e.g tensorflow), but for all intents and purposes these are simply multi-dimensional arrays. \n",
    "\n",
    "If you are familiar with numpy arrays then manipulating pytorch tensors should not be too unfamiliar to you. Below are some examples. Let us begin by importing torch and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a random array of size 2x2x2: NumPy vs PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.26750508, 0.63507303],\n",
       "        [0.72651604, 0.41289983]],\n",
       "\n",
       "       [[0.8224605 , 0.03002775],\n",
       "        [0.9270561 , 0.41149525]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy\n",
    "numpy_random_arr = np.random.rand(2,2,2)\n",
    "numpy_random_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2671, 0.4111],\n",
       "         [0.3481, 0.4226]],\n",
       "\n",
       "        [[0.2356, 0.2893],\n",
       "         [0.7886, 0.5119]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch\n",
    "torch_random_arr = torch.rand(2,2,2)\n",
    "torch_random_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are indexed in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9665), 0.9300530825946917)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_random_arr[0,0,0], numpy_random_arr[0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and can be reshaped using reshape functions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "torch_random_arr = torch_random_arr.reshape(4,2)\n",
    "print(torch_random_arr.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n"
     ]
    }
   ],
   "source": [
    "numpy_random_arr = np.reshape(numpy_random_arr, [4,2])\n",
    "print(numpy_random_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to and from numpy arrays is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]]) # make a numpy array\n",
    "\n",
    "a_torch = torch.from_numpy(a) #converting to a torch Tensor from a numpy array\n",
    "print(a_torch) \n",
    "\n",
    "a_np = a_torch.numpy() # converting to a numpy array from a torch Tensor\n",
    "print(a_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other basic functions such as torch.diag, torch.cat (concatenate), torch.matmul work similarly to their numpy equivalents. <br>\n",
    "As always, when looking for a function **check the documentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that printing the torch tensor also gave a dtype. Just like in numpy, the data type of an object is important.\n",
    "PyTorch Tensors types - just like in any other programming language - depend on whether they are storing integers, floating points or bools, and in how many bits. Often, it is important to make sure tensors are of the right / matching type when performing operations on them.\n",
    "<br>\n",
    "See https://pytorch.org/docs/stable/tensors.html for a list of dtypes and what they are called in PyTorch.\n",
    "\n",
    "Changing torch tensor type is simple too:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(a_torch)\n",
    "print(a_torch.to(torch.double)) #casts the int32 dtype tensor into a 64 bit float dtype tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task: Basic Tensor Operations\n",
    "\n",
    "1. Generate two random numpy arrays, **a** and __b__ of sizes [12,5] and [3,5,20] \n",
    "\n",
    "\n",
    "2. Find the matrix product **a** $\\cdot$ __b__. Result should be of shape (3,12,20) <br>\n",
    "    *hint: may need to reshape a first*\n",
    "    \n",
    "    \n",
    "3. Convert **a** and __b__ into PyTorch Tensors and repeat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 12, 20])\n"
     ]
    }
   ],
   "source": [
    "# Solution:\n",
    "a = np.random.rand(12,5)\n",
    "b = np.random.rand(3,5,20)\n",
    "np.matmul(np.reshape(a, [1, 12,5]),b).shape\n",
    "\n",
    "a_t = torch.from_numpy(a)\n",
    "b_t = torch.from_numpy(b)\n",
    "\n",
    "print(torch.matmul(a_t, b_t).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might imagine, it is not the similarities between the two that we are interested in, but what makes torch Tensors relevant to machine learning. The most significant and relevant difference is that PyTorch Tensors also have an associated *gradient*. It is this that is used to perform the optimization that machine learning is based on. \n",
    "\n",
    "The gradient of a pytorch tensor is stored as its .grad attribute. All pytorch tensors have this even if it is not apparent nor used. In such a case it would be set to \"None\". \n",
    "\n",
    "All PyTorch tensors have another boolean attribute 'requires_grad' that indicates whether pytorch *needs* to track and store its gradient or whether it is simply a static tensor. By default, requires_grad is set to False.\n",
    "When we later construct neural networks from the torch.nn neural network module, requires_grad will be automatically set to True for the relevant learning parameters so it is not something you should generally worry about setting manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(5,5) # generate a random Tensor\n",
    "\n",
    "print(a.grad) # check its gradient - the result is None\n",
    "\n",
    "print(a.requires_grad) # check if its gradient is required - the result is False by default\n",
    "\n",
    "a.required_grad = True # we can set it to True\n",
    "\n",
    "print(a.grad) # but there is still no gradient currently stored, because we havent done anything with it yet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch NN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch.nn module contains all the functions you will need to build a neural network. \n",
    "<br> This includes fully connected layers, convolutions, and pooling operations. \n",
    "\n",
    "\n",
    "It is well documented and easy to read: **See for Yourself** https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "We will go over a few nn modules and then use what we have learned to build an MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin with a 2d Convolution: one of the most common and important functions used in image processing and deep learning in general. We will be generating a 3D 'image' array (3 input channels, 100 x 100 pixels) and performing a 2d Convolution with stride = 1, kernel size = 3x3 and 2 output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.randint(0, 255, (1, 3,100,100)) # our random image. \n",
    "# the first dimension has size N where N is the number of images. here it is simply 1\n",
    "\n",
    "operation = nn.Conv2d(in_channels = 3,out_channels = 2, kernel_size = 3) \n",
    "# building our conv operation. note that we did not need to specify the names of the parameters. nn.Conv2d(3,2,3) is sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "print(operation) #we can see our convolution operation by printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "_thnn_conv2d_forward is not implemented for type torch.LongTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-b10a8f87fa94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: _thnn_conv2d_forward is not implemented for type torch.LongTensor"
     ]
    }
   ],
   "source": [
    "result = operation(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation fails as it cannot work on integer tensors. Let us convert it into a float tensor first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  84.3498,   63.9077,  -17.0368,  ...,  -19.8883,  146.8244,\n",
      "            116.4487],\n",
      "          [ 114.1581,   21.6166,   39.4248,  ...,  120.4760,   40.2672,\n",
      "            124.1527],\n",
      "          [  51.4259,   88.8663,   19.4072,  ...,   27.4615,   47.9384,\n",
      "             31.0113],\n",
      "          ...,\n",
      "          [  69.9386,   80.1651,   97.4229,  ...,   84.3317,   62.7600,\n",
      "             77.0753],\n",
      "          [  59.2197,   80.6550,   56.2093,  ...,   59.6583,   22.5126,\n",
      "             88.3077],\n",
      "          [   4.3444,   -7.2992,  145.9830,  ...,   75.7843,   45.3026,\n",
      "             47.7600]],\n",
      "\n",
      "         [[ -25.0516,   20.1285, -112.5526,  ...,  -54.0992,  -79.3677,\n",
      "            -69.5192],\n",
      "          [ -12.9119,   16.2892,   18.5945,  ...,  -45.2710,  -59.9804,\n",
      "             42.8030],\n",
      "          [ -46.3792,  -84.8874,   21.3314,  ...,  -43.0982, -112.4890,\n",
      "            -54.4308],\n",
      "          ...,\n",
      "          [ -65.6272,   23.0787,   17.7784,  ...,  -90.7418, -109.8219,\n",
      "            -10.4001],\n",
      "          [ -35.5278,  -96.7508,  -27.6198,  ...,  -98.9269,  -28.8617,\n",
      "            -36.9966],\n",
      "          [ -66.1923,  -55.8782,  -86.0278,  ..., -125.9887,    9.7810,\n",
      "            -49.1531]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_image = input_image.to(torch.float)\n",
    "result = operation(input_image)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have our result of a 2d Convolution of our image with some randomly generated kernel. What if we wanted to know what that kernel actually is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight tensor([[[[-0.0503, -0.0771,  0.0385],\n",
      "          [ 0.0618, -0.0579,  0.1223],\n",
      "          [ 0.1245,  0.0250,  0.0932]],\n",
      "\n",
      "         [[-0.1685,  0.0984,  0.1322],\n",
      "          [-0.1506, -0.1639,  0.1860],\n",
      "          [-0.0761,  0.1491, -0.1911]],\n",
      "\n",
      "         [[-0.1289,  0.0783,  0.1418],\n",
      "          [-0.1170,  0.1398,  0.1340],\n",
      "          [ 0.1355, -0.0257,  0.0179]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1175,  0.0906, -0.1641],\n",
      "          [-0.1250, -0.1894,  0.1588],\n",
      "          [-0.0745, -0.0957,  0.0403]],\n",
      "\n",
      "         [[ 0.1258,  0.0379,  0.1697],\n",
      "          [-0.0464,  0.0835, -0.0119],\n",
      "          [-0.1185,  0.1465, -0.0537]],\n",
      "\n",
      "         [[-0.1021, -0.1557,  0.0275],\n",
      "          [-0.0224, -0.0183,  0.1036],\n",
      "          [-0.1804, -0.0144, -0.0969]]]])\n",
      "bias tensor([-0.1075,  0.0405])\n"
     ]
    }
   ],
   "source": [
    "for name, param in operation.named_parameters(): # for each named parameter\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that our convolution weight tensor is of shape [2,3,3] and has a bias of shape [2].\n",
    "\n",
    "We can do the same for a fully connected Linear layer, which can be found in the torch.nn module under the function Linear(). Its parameters are the number of input features and the number of output features.\n",
    "We will use 3x100x100 = 300000 input features and 10 output features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 136.4518,    1.0370,  -72.2169,   -6.4639,   70.8948,   27.0052,\n",
      "          -59.5169,    6.4923, -153.2144,  -13.1952]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "fc_operation = nn.Linear(30000, 10) # defining our fully connected Linear layer\n",
    "\n",
    "reshaped_input_image = input_image.reshape(input_image.size(0), -1) #reshaping input image \n",
    "\n",
    "result = fc_operation(reshaped_input_image) \n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used a strange way to reshape input_image. We specified the dimensions as (input_image.size(0), -1).\n",
    "\n",
    "Remember that the first dimension indicates the number of images. In this simple example there is only one, but in real examples, there can be a variable number of images used in each batch. So by calling size(0) we are keeping this the same. Obviously -1 is invalid as an actual dimension size. It is a very useful feature that tells pytorch to, essentially, figure out itself what should be the corresponding size of this dimension given the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: MaxPooling in 2D.\n",
    "\n",
    "Generate an array of 5 images which have 3 channels are of size (100 x 100) and perform a 2D maxpool on the images using PyTorch. Your max pooling operation should have:\n",
    "\n",
    "1. filter size 3x3, stride = 1 x 1\n",
    "\n",
    "2. filter size 4 x 2, stride = 2 x 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "tensor([[[[229., 235., 235.,  ..., 245., 245., 245.],\n",
      "          [242., 242., 235.,  ..., 245., 245., 245.],\n",
      "          [242., 242., 222.,  ..., 245., 245., 245.],\n",
      "          ...,\n",
      "          [247., 241., 241.,  ..., 246., 246., 246.],\n",
      "          [241., 241., 241.,  ..., 246., 246., 246.],\n",
      "          [241., 241., 241.,  ..., 246., 246., 246.]],\n",
      "\n",
      "         [[248., 170., 188.,  ..., 249., 249., 245.],\n",
      "          [222., 216., 216.,  ..., 249., 249., 220.],\n",
      "          [216., 216., 216.,  ..., 246., 220., 220.],\n",
      "          ...,\n",
      "          [248., 233., 233.,  ..., 191., 231., 231.],\n",
      "          [248., 236., 220.,  ..., 245., 240., 240.],\n",
      "          [247., 236., 220.,  ..., 245., 240., 240.]],\n",
      "\n",
      "         [[246., 252., 252.,  ..., 254., 254., 250.],\n",
      "          [246., 252., 252.,  ..., 254., 250., 250.],\n",
      "          [246., 252., 252.,  ..., 254., 197., 150.],\n",
      "          ...,\n",
      "          [237., 237., 237.,  ..., 247., 247., 247.],\n",
      "          [221., 221., 213.,  ..., 247., 247., 247.],\n",
      "          [221., 237., 237.,  ..., 240., 231., 231.]]],\n",
      "\n",
      "\n",
      "        [[[235., 222., 176.,  ..., 218., 250., 250.],\n",
      "          [235., 213., 213.,  ..., 200., 250., 250.],\n",
      "          [215., 213., 231.,  ..., 218., 203., 216.],\n",
      "          ...,\n",
      "          [226., 226., 226.,  ..., 199., 226., 226.],\n",
      "          [226., 226., 226.,  ..., 220., 220., 213.],\n",
      "          [226., 212., 212.,  ..., 233., 233., 233.]],\n",
      "\n",
      "         [[251., 244., 244.,  ..., 253., 253., 211.],\n",
      "          [251., 244., 244.,  ..., 253., 253., 250.],\n",
      "          [251., 244., 244.,  ..., 253., 253., 250.],\n",
      "          ...,\n",
      "          [191., 177., 140.,  ..., 207., 239., 239.],\n",
      "          [246., 177., 140.,  ..., 203., 239., 239.],\n",
      "          [246., 177., 233.,  ..., 218., 239., 239.]],\n",
      "\n",
      "         [[248., 248., 248.,  ..., 248., 248., 248.],\n",
      "          [248., 248., 250.,  ..., 244., 244., 239.],\n",
      "          [254., 254., 250.,  ..., 241., 241., 239.],\n",
      "          ...,\n",
      "          [225., 235., 235.,  ..., 229., 229., 228.],\n",
      "          [208., 235., 235.,  ..., 187., 213., 246.],\n",
      "          [235., 208., 208.,  ..., 246., 246., 246.]]],\n",
      "\n",
      "\n",
      "        [[[245., 167., 168.,  ..., 238., 238., 238.],\n",
      "          [232., 232., 167.,  ..., 238., 238., 238.],\n",
      "          [232., 232., 221.,  ..., 212., 238., 238.],\n",
      "          ...,\n",
      "          [239., 249., 249.,  ..., 233., 233., 204.],\n",
      "          [237., 249., 249.,  ..., 234., 244., 244.],\n",
      "          [241., 249., 249.,  ..., 234., 244., 244.]],\n",
      "\n",
      "         [[245., 245., 245.,  ..., 239., 239., 223.],\n",
      "          [245., 245., 245.,  ..., 239., 239., 205.],\n",
      "          [245., 245., 245.,  ..., 244., 193., 234.],\n",
      "          ...,\n",
      "          [254., 254., 254.,  ..., 206., 208., 223.],\n",
      "          [254., 254., 254.,  ..., 224., 206., 223.],\n",
      "          [254., 254., 254.,  ..., 224., 214., 232.]],\n",
      "\n",
      "         [[231., 231., 231.,  ..., 240., 234., 234.],\n",
      "          [234., 231., 235.,  ..., 240., 234., 234.],\n",
      "          [234., 231., 235.,  ..., 240., 222., 222.],\n",
      "          ...,\n",
      "          [242., 241., 241.,  ..., 201., 201., 244.],\n",
      "          [242., 241., 241.,  ..., 204., 204., 234.],\n",
      "          [241., 241., 241.,  ..., 251., 251., 251.]]],\n",
      "\n",
      "\n",
      "        [[[220., 220., 164.,  ..., 244., 252., 252.],\n",
      "          [221., 221., 229.,  ..., 244., 244., 211.],\n",
      "          [221., 221., 229.,  ..., 246., 248., 248.],\n",
      "          ...,\n",
      "          [241., 237., 183.,  ..., 229., 210., 196.],\n",
      "          [211., 211., 211.,  ..., 240., 240., 216.],\n",
      "          [211., 211., 211.,  ..., 240., 240., 237.]],\n",
      "\n",
      "         [[227., 227., 252.,  ..., 235., 235., 235.],\n",
      "          [227., 227., 252.,  ..., 235., 235., 235.],\n",
      "          [227., 227., 252.,  ..., 235., 235., 235.],\n",
      "          ...,\n",
      "          [245., 190., 226.,  ..., 247., 211., 211.],\n",
      "          [245., 242., 242.,  ..., 247., 211., 211.],\n",
      "          [245., 242., 242.,  ..., 222., 222., 222.]],\n",
      "\n",
      "         [[254., 254., 183.,  ..., 236., 253., 253.],\n",
      "          [254., 254., 196.,  ..., 233., 233., 233.],\n",
      "          [254., 254., 196.,  ..., 241., 241., 241.],\n",
      "          ...,\n",
      "          [223., 248., 248.,  ..., 253., 253., 253.],\n",
      "          [207., 233., 233.,  ..., 253., 253., 253.],\n",
      "          [207., 233., 233.,  ..., 253., 253., 253.]]],\n",
      "\n",
      "\n",
      "        [[[254., 254., 254.,  ..., 239., 239., 239.],\n",
      "          [243., 235., 235.,  ..., 232., 232., 250.],\n",
      "          [243., 235., 235.,  ..., 232., 232., 253.],\n",
      "          ...,\n",
      "          [233., 247., 247.,  ..., 252., 252., 246.],\n",
      "          [237., 247., 247.,  ..., 197., 197., 246.],\n",
      "          [237., 237., 237.,  ..., 253., 230., 230.]],\n",
      "\n",
      "         [[229., 229., 229.,  ..., 201., 239., 245.],\n",
      "          [229., 229., 229.,  ..., 228., 239., 245.],\n",
      "          [229., 229., 229.,  ..., 241., 242., 242.],\n",
      "          ...,\n",
      "          [184., 202., 234.,  ..., 241., 209., 223.],\n",
      "          [240., 207., 234.,  ..., 241., 209., 223.],\n",
      "          [251., 251., 251.,  ..., 238., 209., 223.]],\n",
      "\n",
      "         [[253., 224., 224.,  ..., 246., 246., 246.],\n",
      "          [224., 254., 254.,  ..., 246., 252., 252.],\n",
      "          [203., 254., 254.,  ..., 253., 253., 253.],\n",
      "          ...,\n",
      "          [245., 245., 245.,  ..., 254., 225., 225.],\n",
      "          [245., 245., 245.,  ..., 254., 232., 156.],\n",
      "          [245., 245., 245.,  ..., 249., 243., 243.]]]])\n",
      "MaxPool2d(kernel_size=(4, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "tensor([[[[242., 235., 228.,  ..., 201., 245., 238.],\n",
      "          [242., 222., 251.,  ..., 201., 245., 136.],\n",
      "          [245., 111., 251.,  ..., 172., 194., 171.],\n",
      "          ...,\n",
      "          [247., 208., 123.,  ..., 238., 180., 234.],\n",
      "          [247., 241., 106.,  ..., 231., 246., 211.],\n",
      "          [209., 241., 159.,  ..., 211., 246., 230.]],\n",
      "\n",
      "         [[248., 216., 254.,  ..., 246., 249., 245.],\n",
      "          [247., 216., 254.,  ..., 246., 220., 175.],\n",
      "          [247., 182., 245.,  ..., 254., 231., 175.],\n",
      "          ...,\n",
      "          [249., 234., 246.,  ..., 247., 191., 248.],\n",
      "          [249., 233., 246.,  ..., 247., 191., 231.],\n",
      "          [248., 220., 232.,  ..., 245., 240., 237.]],\n",
      "\n",
      "         [[246., 252., 245.,  ..., 254., 254., 191.],\n",
      "          [246., 252., 196.,  ..., 254., 197., 173.],\n",
      "          [228., 238., 241.,  ..., 226., 239., 240.],\n",
      "          ...,\n",
      "          [233., 237., 235.,  ..., 192., 173., 228.],\n",
      "          [233., 237., 235.,  ..., 245., 247., 243.],\n",
      "          [221., 237., 253.,  ..., 245., 247., 243.]]],\n",
      "\n",
      "\n",
      "        [[[235., 213., 248.,  ..., 160., 218., 250.],\n",
      "          [215., 213., 240.,  ..., 218., 203., 216.],\n",
      "          [200., 206., 240.,  ..., 218., 250., 218.],\n",
      "          ...,\n",
      "          [254., 246., 230.,  ..., 240., 253., 235.],\n",
      "          [226., 242., 203.,  ..., 231., 181., 226.],\n",
      "          [226., 226., 222.,  ..., 253., 233., 213.]],\n",
      "\n",
      "         [[251., 244., 235.,  ..., 242., 253., 250.],\n",
      "          [251., 244., 235.,  ..., 253., 253., 250.],\n",
      "          [248., 254., 242.,  ..., 253., 236., 228.],\n",
      "          ...,\n",
      "          [191., 191., 254.,  ..., 213., 207., 192.],\n",
      "          [191., 191., 254.,  ..., 213., 207., 239.],\n",
      "          [246., 155., 233.,  ..., 218., 203., 239.]],\n",
      "\n",
      "         [[192., 248., 250.,  ..., 249., 248., 239.],\n",
      "          [254., 248., 250.,  ..., 222., 241., 239.],\n",
      "          [254., 247., 207.,  ..., 229., 209., 233.],\n",
      "          ...,\n",
      "          [232., 248., 214.,  ..., 244., 250., 228.],\n",
      "          [196., 248., 205.,  ..., 244., 229., 228.],\n",
      "          [235., 235., 205.,  ..., 139., 246., 246.]]],\n",
      "\n",
      "\n",
      "        [[[245., 167., 196.,  ..., 220., 238., 238.],\n",
      "          [232., 221., 243.,  ..., 221., 212., 238.],\n",
      "          [228., 221., 243.,  ..., 247., 193., 253.],\n",
      "          ...,\n",
      "          [246., 239., 202.,  ..., 233., 178., 229.],\n",
      "          [223., 249., 228.,  ..., 231., 233., 199.],\n",
      "          [237., 249., 228.,  ..., 231., 234., 244.]],\n",
      "\n",
      "         [[190., 245., 247.,  ..., 223., 239., 223.],\n",
      "          [200., 245., 232.,  ..., 244., 193., 234.],\n",
      "          [200., 248., 232.,  ..., 244., 191., 234.],\n",
      "          ...,\n",
      "          [250., 200., 224.,  ..., 236., 203., 208.],\n",
      "          [198., 254., 241.,  ..., 236., 206., 223.],\n",
      "          [198., 254., 241.,  ..., 224., 214., 232.]],\n",
      "\n",
      "         [[234., 231., 235.,  ..., 240., 234., 170.],\n",
      "          [234., 231., 235.,  ..., 240., 227., 235.],\n",
      "          [241., 173., 240.,  ..., 250., 227., 235.],\n",
      "          ...,\n",
      "          [228., 153., 246.,  ..., 251., 249., 244.],\n",
      "          [242., 241., 246.,  ..., 251., 249., 244.],\n",
      "          [242., 241., 215.,  ..., 177., 251., 234.]]],\n",
      "\n",
      "\n",
      "        [[[220., 221., 241.,  ..., 242., 244., 252.],\n",
      "          [194., 221., 229.,  ..., 200., 246., 248.],\n",
      "          [194., 182., 189.,  ..., 200., 246., 248.],\n",
      "          ...,\n",
      "          [241., 208., 225.,  ..., 234., 224., 252.],\n",
      "          [241., 183., 225.,  ..., 234., 224., 252.],\n",
      "          [ 80., 211., 174.,  ..., 240., 240., 237.]],\n",
      "\n",
      "         [[170., 227., 252.,  ..., 246., 235., 235.],\n",
      "          [209., 227., 252.,  ..., 234., 235., 235.],\n",
      "          [209., 172., 173.,  ..., 234., 254., 219.],\n",
      "          ...,\n",
      "          [224., 209., 215.,  ..., 225., 204., 240.],\n",
      "          [245., 201., 226.,  ..., 247., 204., 240.],\n",
      "          [245., 242., 226.,  ..., 247., 222., 211.]],\n",
      "\n",
      "         [[254., 196., 241.,  ..., 249., 236., 253.],\n",
      "          [254., 196., 233.,  ..., 249., 241., 233.],\n",
      "          [251., 235., 233.,  ..., 240., 241., 194.],\n",
      "          ...,\n",
      "          [234., 249., 241.,  ..., 221., 207., 213.],\n",
      "          [234., 249., 242.,  ..., 252., 253., 252.],\n",
      "          [169., 233., 242.,  ..., 252., 253., 252.]]],\n",
      "\n",
      "\n",
      "        [[[243., 254., 230.,  ..., 216., 239., 250.],\n",
      "          [250., 235., 230.,  ..., 168., 232., 253.],\n",
      "          [250., 212., 252.,  ..., 234., 202., 253.],\n",
      "          ...,\n",
      "          [200., 243., 241.,  ..., 253., 252., 186.],\n",
      "          [233., 247., 241.,  ..., 247., 252., 246.],\n",
      "          [233., 247., 156.,  ..., 253., 230., 246.]],\n",
      "\n",
      "         [[226., 229., 219.,  ..., 240., 228., 245.],\n",
      "          [226., 229., 219.,  ..., 230., 241., 242.],\n",
      "          [243., 214., 171.,  ..., 230., 241., 242.],\n",
      "          ...,\n",
      "          [247., 160., 244.,  ..., 234., 235., 180.],\n",
      "          [247., 202., 238.,  ..., 241., 235., 223.],\n",
      "          [240., 251., 236.,  ..., 241., 194., 223.]],\n",
      "\n",
      "         [[253., 254., 192.,  ..., 252., 246., 252.],\n",
      "          [202., 254., 235.,  ..., 252., 253., 252.],\n",
      "          [250., 181., 235.,  ..., 249., 253., 231.],\n",
      "          ...,\n",
      "          [230., 245., 211.,  ..., 185., 226., 249.],\n",
      "          [230., 245., 243.,  ..., 254., 225., 249.],\n",
      "          [180., 245., 243.,  ..., 254., 232., 243.]]]])\n"
     ]
    }
   ],
   "source": [
    "#solution\n",
    "\n",
    "random_ims = torch.randint(0, 255, (5,3,100,100)).to(torch.float)\n",
    "\n",
    "maxpoolop = nn.MaxPool2d(3,1) \n",
    "print(maxpoolop)\n",
    "r = maxpoolop(random_ims)\n",
    "print(r)\n",
    "\n",
    "\n",
    "maxpoolop = nn.MaxPool2d((4,2),2) \n",
    "print(maxpoolop)\n",
    "r = maxpoolop(random_ims)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Before we can use a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "### criterion and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
